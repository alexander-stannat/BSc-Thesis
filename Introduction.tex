\documentclass[11pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\pagestyle{plain}
\pagenumbering{arabic}
\usepackage[margin=0.5cm]{geometry}
\topmargin -1cm
\textheight 25cm
\textwidth 16.0 cm
\oddsidemargin 0.2cm
\graphicspath{ {C:\Users\alexa\Dropbox\Bachelorarbeit} }
\begin{document}
\pagenumbering{gobble}
\bigskip\noindent
\hrule\vspace{1em}
\begin{center}{\bf{\Large \textsc{Distances and metrics on probability measures}}}\end{center}\vspace{1em}
\hrule
\bigskip\bigskip\bigskip\noindent 
\begin{center}
{\bf Bachelor thesis} \vspace{1em}\\ written by \vspace{1em}\\ Alexander Stannat \\ \bigskip at the \vspace{3em}\\ \includegraphics[scale=1.3]{Bild.png}  \\
\bigskip\bigskip Chair of mathematics and statistics \\ \bigskip supervised by \vspace{1em}\\ Prof. Dr. Michael Kupper \vspace{2em}\\ Konstanz \\ 16.03.2017
\end{center}

\newpage

\setcounter{tocdepth}{2}
\tableofcontents
\newpage
\pagenumbering{arabic}
\section{Introduction}
In mathematics the concept of distance has a very important role to play. From analysis to algebra the notion of difference between two mathematical entities can not be gone without. In order to quantify distance, mathematics provides us with the useful definition of a metric. A metric is a mapping, taking two elements of a set and assigning them to their mutual distance from each other. More specifically, a metric is a symmetric and positive semidefinite mapping from the product space of the mathematical entities in question to $\mathbb{R}_{\geq{}0}$, satisfying the triangle inequality. A particular case of a metric is a norm, assigning an object to its distance from zero. In probability theory, a statistical distance quantifies the distance between two statistical objects, such as random variables, probability distributions, etc. Statistical distances endow the space of the statistical objects in question with a topology, giving way to concepts such as openness and closedness, which entail functionalanalytical principles such as convergence and density, etc. \vspace{1em}\\
In this transcript we will introduce the idea of probability distances, also referred to as probability metrics, if the aforementioned conditions of a metric are complied with. Note, that in this transcript we will use the terminologies of distance and metric equivalently. Given a measurable space and the set of probability measures on its $\sigma$-algebra, these metrics assign two probability measures to their mutual distance from eachother, creating some notion of distance on the space of probability measures as well as endowing it with a topology. The main value in these metrics lies in the structure that they create on the space of probability measures. In many cases the topology induced, equates to the topology of weak convergence or to an even greater one and additionally some of the structure of the underlying measurable space is transferred to the space of probability measures. There is a wide variety of probability metrics to examine and we will introduce the reader to the most important ones. \vspace{1em}\\
The reason behind the multitude of  probability distances is best explained as follows. Imagine wanting to find out the distance between Berlin and Shanghai. This is an easy task. All we need to do is find a path connecting the two, the length of which yields one version of distance. The point is that in a case like this it has been conventionalised to take the length of the shortest path connecting the two points, which doesn't leave much room for alternative interpretations of distance. Now imagine being asked for the distance between several cities in Asia and Europe, or for that matter for the distance between Europe and Asia in general. Suddenly the concept of distance is no longer a clearly defined, tangible idea. There are an infinte number of ways of defining such distance. In this case, a useful approach, would be to interpret the spatial distributions of the towns in Europe and Asia (or Europe and Asia themselves) by two probability measures. Now we are asking for the distance between the probability measures, i.e. a probability distance.\vspace{1em}\\
This idea is closely related to the concept of the optimal transport problem, which we will begin with, in this transcript. After an extensive introduction to the Monge-Kantorovich problem, we move on to introducing the Wasserstein distance and the Total Variation distance as a special case as well as an upper bound of the Wasserstein distance. After a comprehensive insight into these probability metrics, we introduce the Prokhorov metric and as a special case the Lévy metric. The respective applications of these probability distances in terms of the topology that they induce on the space of probability measures as well as their relation to the topology of weak convergence, will constitute our main focus among a few bounding properties. Finally we finish by providing the reader with a superficial outlook on the most common metrics and their respective topologies.\newpage
\section{Optimal transport}
The field of optimal transport is a fairly old one and the idea of transporting something from one place to another, while minimising the cost is still present to this day. It dates back to the $18^{\text{th}}$ century, when the the French mathematician Gaspard Monge published one of his famous works entitled {\sl Mémoire sur la théorie des déblais et des remblais} \cite{Monge}, which was based on the hypothetical problem of having to transport a certain amount of soil extracted from a pit to a construction site. In this case a {\sl déblai} is an amount of mass, extracted from the ground, while a {\sl remblai} is mass integrated in some construction site. We will broaden this concept further in the following chapter.
\subsection{The Monge problem}
The concept of optimal transport is best explained in the context of Monge's optimal transport problem, mentioned above. We will rephrase this problem with an analogy, involving a consortium of cafés and bakeries. Note, that this transcript is loosely based on Cedric Villani's script entitled {\sl Optimal Transport, Old and New} \cite{Villani}.\vspace{1em}\\
Let's suppose we are given a consortium of $n$ bakeries and $m$ cafés. Let's also assume for the sake of the argument that the set of all bakeries $\mathcal{X}$ and the set of cafés $\mathcal{Y}$ form two disjoint subsets of the euclidean plane $\mathbb{R}^2$. One can visualise this, by picturing a map from above with points scattered across it, marking the individual bakeries and cafés. The bakeries, of course produce bread and deliver it to the cafés, that go on to sell it. Since the delivery of bread isn't free, there exists a cost function $c:\mathcal{X\times{}Y}\rightarrow\mathbb{R}_{+}$, where $c(x,y)$ denotes the price of delivering one unit of bread from bakery $x$ to café $y$. \vspace{1em}\\
The cost of transportation obviously depends on the spatial distributions of the bakeries and the cafés, i.e. a function that tells us how much bread is produced by bakeries in some area of our "map" and another function to indicate how much bread is consumed by cafés in another area. This is best done by a measure, but before detailing what a measure is, we must first introduce the concept of a $\sigma$-algebra.
\subsubsection{Definition}
Given a set $\mathcal{X}$ we call a set of subsets $\mathcal{F}$ of $\mathcal{X}$ a $\sigma$-algebra, if it satisfies the following conditions \vspace{0.25em}
\begin{align*}
(i)&\,\,\mathcal{X}\in\mathcal{F} \\[6pt](ii)&\,\,A\in\mathcal{F}\implies A^c\in\mathcal{F} \\[3pt](iii)&\,\,A_1,A_2,A_3,\ldots\in\mathcal{F}\implies\bigcup\limits_{n\in\mathbb{N}}A_n\in\mathcal{F}
\end{align*}
In that case we call $(\mathcal{X},\mathcal{F})$ a measurable space.
If $(\mathcal{X},d)$ is a metric space, one usually restricts one's attention to the Borel $\sigma$-algebra. The Borel $\sigma$-algebra is defined over the topology, induced by $d$, whereby we take the smallest $\sigma$-algebra that contains all open sets. With this in mind we know that any metric space is already a measurable space. In our case we mostly focus on Borel $\sigma$-algebras on metric spaces.\vspace{2em}\\
With respect to the aforementioned spatial distributions of the cafés and bakeries, we introduce the idea of a measure. 
\subsubsection{Definition}
Given a measurable space $(\mathcal{X,F})$, a $\sigma$-additive function $\mu{}:\mathcal{F}\rightarrow{}[0,\infty]$ is called a measure, if it satisfies $\mu(\varnothing)=0$.\\ A function $\mu$ is called $\sigma$-additive, if for a sequence of disjoint sets $A_1,A_2,A_3,\ldots\in\mathcal{F},$ it holds 
\begin{align*}
\mu(\dot{\bigcup\limits_{n\in\mathbb{N}}}A_n)=\sum\limits_{n\in\mathbb{N}}\mu(A_n).
\end{align*}
A measure $\mu$ is called a probability measure, if additionally it satisfies $\mu(\mathcal{X})=1$. We then call $(\mathcal{X},\mathcal{F},\mu)$ a probability space.
The set of all probability measures on some measurable space $\mathcal{X}$ is symbolised by $\mathcal{P(X)}$.\\
Oftentimes we will simply write $(\mathcal{X},\mu)$ instead of $(\mathcal{X},\mathcal{F},\mu)$, if $\mathcal{F}$ can be derived from the context.\vspace{1em}\\
Applying these definitions to the preceding "café bakery" analogy, the original Monge transport problem takes on the following form.\vspace{1em}\\ We're given two probability spaces $(\mathcal{X},\mathcal{F_X},\mu)$ and $(\mathcal{Y},\mathcal{F_Y},\nu)$, whereby $\mathcal{X}$ denotes the set of bakeries and $\mathcal{Y}$ the set of cafés. $\mu$ and $\nu$ are the respective spatial distributions of the bakeries and cafés within $\mathcal{X}$ and $\mathcal{Y}$. That means for $A\in\mathcal{F_X},\,\mu(A)$ signifies the proportion of bread that is supplied by bakeries in $A$. Analogously, for $B\in\mathcal{Y},\,\nu(B)$ denotes the proportion of bread consumed by cafés in $B$. It is important to note here that $\mu$ and $\nu$ don't quantify the amount of bread produced/consumed itself, since probability measures are bounded by 1. Instead we define them to denote the proportion of bread, whereby the proportion of bread is simply given by the amount of units of bread produced in $A$ or consumed in $B$ divided by the amount of bread produced/consumed in general. For the notion of cost to remain viable we need to change the cost function to denote the cost of delivery of one unit mulitplied by the consumption/production rate. Given the fact that this has virtually no effect on the setting of the problem itself it will be silently implied and no longer mentioned. \vspace{2em}\\
Take a transport function $T:\mathcal{X}\rightarrow\mathcal{Y},$ with $T(x)$ being the café that bakery $x$ delivers its bread to. Obviously $T$ then needs to be measurable and the supply and demand of the individual cafés and bakeries need to be satisfied. For $T$, this implies that for any $A\in\mathcal{F_X}$, the cafés in $T(A)$ consume just as much bread as bakeries in $A$ produce. This means $\nu(T(A))=\mu(A)$ or more generally we require 
\[
\mu\circ{}T^{-1}=\nu.
\vspace{1em}\\
\]
Given a cost function $c:\mathcal{X\times{}Y}\rightarrow\mathbb{R}_{+}$, Monge's optimal transport problem lies in finding a measurable transport function $T$, that minimises the total cost of transportation, given by 
\begin{align*}
\int\limits_{\mathcal{X}}c(x,T(x))d\mu(x).
\end{align*} 
The problem we face now, is that for $T$ to be well-defined, one bakery can only deliver bread to one café. It is not allowed for a bakery to split its production "mass", i.e to deliver to several cafés simultaneously. This makes finding a transport function between the two given probability spaces not always feasible.\vspace{1em}\\Let's say, for instance $\mu$ is a dirac measure and $\nu$ is not, then the requirement
\[
\mu\circ{}T^{-1}=\nu
\]
can not be satisfied for any well-defined $T$. This can simply be interpreted as there being only one bakery and several cafés in need of supply.\vspace{1em}\\
We need a more general approach to the problem. With this in mind, we move on from finding a transport function $T$, to trying to find a coupling between the two probability measures $\mu$ and $\nu$.
\subsection{Couplings}
Coupling is a very useful proof technique, that allows us to compare two unrelated random distributions.
\subsubsection{Definition}
Let $\mu$ and $\nu$ be two probability measures on some measurable spaces $(\mathcal{X,F}_1)$ and $(\mathcal{Y,F}_2)$. Coupling $\mu$ and $\nu$ means contructing a probability space $(\Omega,\mathbb{P})$ and a tuple of random variables $(X,Y):\Omega\rightarrow\mathcal{X\times{}Y}$, such that 
\begin{align*}
&\text{law}(X):=\mathbb{P}\circ{}X^{-1}=\mu \\[1.5pt]&\text{law}(Y):=\mathbb{P}\circ{}Y^{-1}=\nu.
\end{align*}
We call the tuple $(X,Y)$ a coupling of $(\mu,\nu)$. By abuse of language we call the law of $(X,Y)$ a coupling of $(\mu,\nu)$ as well. \vspace{1em}\\
It is clear, that for any two probability measures $\mu$ and $\nu$, there always exists a coupling, since one can simply set $\Omega:=\mathcal{X\times{}Y}$ and $\mathbb{P}:=\mu\otimes\nu$. The coupling of $(\mu,\nu)$ given by the tuple of random variables $(\text{proj}_{\mathcal{X}},\text{proj}_{\mathcal{Y}})$, then simply equates to the product measure, which obviously always exists. For a coupling $\pi$ of $(\mu,\nu)$ it holds 
\[
\pi(A\times\mathcal{Y}) = \mu(A)\quad\text{and}\quad\pi(\mathcal{X}\times{}B)=\nu(B).
\]
This is known as the marginal condition and can be rephrased in the following two equivalent ways:
\begin{itemize}
\item[$(i)$] \quad For all integrable functions $\psi:\mathcal{X}\rightarrow\mathbb{R}$ and $\phi:\mathcal{Y}\rightarrow\mathbb{R}$ it holds 
\[
\int\limits_{\mathcal{X\times{}Y}}\psi(x)+\phi(y)\,d\pi(x,y) = \int\limits_{\mathcal{X}}\psi(x)\,d\mu(x) + \int\limits_{\mathcal{Y}}\phi(y)\,d\nu(y).
\]
\item[$(ii)$] \quad $\pi\circ{}\text{proj}_{\mathcal{X}}^{-1} = \mu$\quad and\quad $\pi\circ{}\text{proj}_{\mathcal{Y}}^{-1} = \nu.$
\end{itemize}
\subsubsection{Definition}
A deterministic coupling of two probability measures $(\mu,\nu)$ is given by a probability space $(\Omega,\mathbb{P})$ and a tuple of random variables $(X,Y)$ as well as a measurable function $T:\mathcal{X}\rightarrow\mathcal{Y}.$ In that case $(X,Y):\Omega\rightarrow\mathcal{X\times{}Y}$ is a called a deterministic coupling of $(\mu,\nu)$, if one of the following equivalent conditions is satisfied: 
\begin{itemize}
\item[$(i)$]\begin{align*}
&\mathbb{P}\circ{}X^{-1}=\mu,\,\mathbb{P}\circ{}Y^{-1}=\nu\,\,\text{ and }\,\, T\circ{}X=Y,\\\text{ then }\,\,&\mu\circ{}T^{-1}=\mathbb{P}\circ{}X^{-1}\circ{}T^{-1}=\mathbb{P}\circ{}(T\circ{}X)^{-1}=\mathbb{P}\circ{}Y^{-1}=\nu. 
\end{align*}
\item[$(ii)$]\begin{align*}
\mathbb{P}\circ{}(X,Y)^{-1} \text{ is concentrated on the graph of T }\,\, \lbrace (x,T(x)); x\in\mathcal{X}\rbrace. 
\end{align*}
\item[$(iii)$]\begin{align*}
\text{ For all } & \text{ integrable  functions }\, \phi:\mathcal{Y}\rightarrow\mathbb{R}\, \text{ it holds }\\&
\int\limits_{\mathcal{Y}}\phi(y)d\nu(y) = \int\limits_{\mathcal{X}}\phi(T(x))d\mu(x).
\end{align*}
\item[$(iv)$]\begin{align*}
(\mu\otimes\nu)=\mu\circ{}(\text{Id},T)^{-1}.
\end{align*}
\end{itemize}
An interesting example is coupling a dirac measure with another probability measure.
\subsubsection{Theorem}
Given two probability spaces $(\mathcal{X},\delta_{x_0})$ and $(\mathcal{Y},\nu)$ for an arbitrary $x_0\in\mathcal{X}$, then the product measure 
$\delta_{x_0}\otimes\nu$ is the only coupling of the two measures. 
\begin{proof}
We will show this by taking a coupling $(X,Y)$ with a probability space $(\Omega,\mathbb{P})$ such that $\pi:=\mathbb{P}\circ{}(X,Y)^{-1}$ has marginals $\delta_{x_0}$ and $\nu$.\vspace{1em}\\ Assume now $\pi\neq\delta_{x_0}\otimes\nu$, then there exists $A\times{}B\in\mathcal{X\times{}Y}$ such that $\pi(A\times{}B)\neq\delta_{x_0}(A)\nu(B)$. This leaves us with two possible cases\vspace{1em}\\
Case 1: $(x_0\not\in{}A)$\vspace{0.5em}\\
$\pi(A\times{}B)\leq\pi(A\times\mathcal{Y})=\delta_{x_0}(A)=0$, which implies $\pi(A\times{}B)=0=\delta_{x_0}(A)\nu(B).$\vspace{1em}\\
Case 2: $(x_0\in{}A)$\vspace{0.5em}\\
$\pi(A\times{}B)=\pi(\mathcal{X}\times{}B)-\pi(A^c\times{}B)=\nu(B)-0=\nu(B) = \delta_{x_0}(A)\nu(B).$\vspace{1em}\\
Hence, we know that any coupling $\pi$ of $(\delta_{x_0},\nu)$ is already equal to the product measure. 
\end{proof}
\subsection{The Monge-Kantorovich problem}
In the original Monge problem we had the objective of trying to find a deterministic coupling to minimise the cost of transportation. Since this wasn't always practicable, we now change the approach to finding a coupling of the two probability measures $\mu$ and $\nu$. In this case couplings describe transportation plans, (i.e $\pi(A,B)$ denotes the proportion of bread delivered from bakeries in a measurable set $A\subset\mathcal{X}$ to cafés in a measurable set $B\subset\mathcal{Y}$). It is now our goal to minimise the transportation cost over all possible transportation plans. This brings us to the Monge-Kantorovich problem. Recall the definition of a Polish space.
\subsubsection{Definition}
A topological space $\mathcal{X}$ is called complete if every Cauchy sequence converges in $\mathcal{X}$. It is called separable, if there exists a countable and dense subset. A metric space $(\mathcal{X},d)$ is called a Polish space, if it is complete and separable.\vspace{1em}\\
Take two Polish spaces $(\mathcal{X},\mu)$ and $(\mathcal{Y},\nu)$, whereby $\mu$ and $\nu$ are probability measures on the Borel $\sigma$-algebras, induced by the respective metrics on $\mathcal{X}$ and $\mathcal{Y}$, describing the spatial distributions of the cafés and bakeries. Let $c:\mathcal{X\times{}Y}\rightarrow\mathbb{R}_{+}$ be a cost function, then the price of a transport plan, described by a coupling $\pi$ of $\mu$ and $\nu$, is given by 
\begin{align*}
C:=\int\limits_{\mathcal{X\times{}Y}}c(x,y)d\pi(x,y).
\end{align*}
It is now our goal to find an optimal coupling $\pi$ of $\mu$ and $\nu$ that minimises the transportation costs. We call couplings $\pi$ of $(\mu,\nu)$ transference plans, whereby the marginal condition of $\pi$ can be interpreted as the fact that a workable transference plan saturates the supply of the bakeries and the demand of the cafés. Let $\Pi(\mu,\nu)$ denote the set of all couplings of $(\mu,\nu)$, then it is our goal to find
\begin{align}
\inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}Y}}c(x,y)d\pi(x,y).
\label{Optimal transport cost}
\end{align}
(\ref{Optimal transport cost}) takes on another form, given by the Kantorovich duality, that will prove itself useful in the future. This brings us to a few lemmas and theorems, which we will not prove here for the most part. The proofs to these can be found in \cite{Villani}.
\subsubsection{Definition}
Let $(\mathcal{X},d)$ be a metric space and let $\mathcal{P(X)}$ be the set of all probability measures on the Borel $\sigma$-algebra of $\mathcal{X}$, induced by the metric $d$. We then define the topology of weak convergence as the coarsest topology on $\mathcal{P(X)}$ such that for all open $G\subset\mathcal{X}$, the mapping $\mathcal{P(X)}\rightarrow{}[0,1],\mu\mapsto\mu(G)$ is lower semicontinuous.\vspace{1em}\\ As the name would suggest, this topology admits a convergence, namely weak convergence. We call a sequence, that converges in this topology weakly convergent, denoted $\mu_n\,\substack{\omega \\ \longrightarrow}\,\mu.$
\subsubsection{Theorem (Portmanteau)}
Given a sequence $(\mu_n)_{n\in\mathbb{N}}\subset{}\mathcal{P(X)}$ and $\mu\in{}\mathcal{P(X)},$ then the following are equivalent\\
\begin{align*}
(i)&\hspace{1em}\mu_n\,\substack{\omega \\ \longrightarrow}\,\mu \\[10pt]
(ii)&\hspace{1em}\text{For every open }\, G\subset\mathcal{X} \,\text{ we have }\,\liminf\limits_{n\rightarrow\infty}\mu_n(G)\geq\mu(G)\\
(iii)&\hspace{1em}\text{For every }\, f\in{}C_b(\mathcal{X})\,\text{ we have }\,\lim\limits_{n\rightarrow\infty}\int\limits_{\mathcal{X}}f(x)d\mu_n(x) = \int\limits_{\mathcal{X}}f(x)d\mu(x).
\end{align*}   
\subsubsection{Lemma}
Let $\mathcal{X}$ and $\mathcal{Y}$ be two Polish spaces then the set of all couplings $\Pi(\mu,\nu)$ of $(\mu,\nu)$ is closed in the topology of weak convergence. 
\subsection{Existence of an optimal coupling}
The given problem consists in minimising the cost of transportation. The first question that arises, pertaining to the cost of the optimal transport plan is, is there actually a transference plan that minimises the cost. It turns out that the the infimum over all couplings in (\ref{Optimal transport cost}) is attained and therefore a minimum.
\subsubsection{Theorem}
Let $(\mathcal{X},\mu)$ and $(\mathcal{Y},\nu)$ be two Polish probability spaces, let 
\[
a:\mathcal{X}\rightarrow\mathbb{R}\cup\lbrace{}-\infty\rbrace \quad\text{ and }\quad b:\mathcal{Y}\rightarrow\mathbb{R}\cup\lbrace{}-\infty\rbrace
\]
be two upper semicontinuous functions such that $a\in{}L^1(\mu),b\in{}L^1(\nu).$ \vspace{1em}\\
Given a lower semicontinuous cost function $c:\mathcal{X\times{}Y}\rightarrow\mathbb{R}_{+}\cup\lbrace\infty\rbrace$ with $c(x,y)\geq{}a(x)+b(y)$ for all $(x,y)\in\mathcal{X\times{}Y}$, there exists a coupling $\tilde{\pi}$ of $(\mu,\nu)$, such that
\[
\inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}Y}}c(x,y)d\pi(x,y) = \int\limits_{\mathcal{X\times{}Y}}c(x,y)d\tilde{\pi}(x,y).
\]
It goes without saying, that this is a very useful conclusion to come to. Especially, since the conditions, on which the infimum is attained are farily mild. For instance, if one chooses a metric as the cost function, we know there exists a minimising transport plan. 
From this we move on to the Kantorovich duality.
\section{The Kantorovich duality}
Suppose now we find ourselves in the following situation:\\
We have come up with a transportation plan, delivering bread from bakeries in $\mathcal{X}$ to cafés in $\mathcal{Y}$. Wanting to reduce the cost of delivery, we take a bakery $x_1$ and reroute one unit of bread, that was originally sent to some café $y_1$, to a closer café $y_2$. We then gain $c(x_1,y_2)-c(x_1,y_1)$. Since there is now an excess of bread delivered to $y_2$, we reroute one unit of the bread, sent from some bakery $x_2$ to café $y_2$ to another café $y_3$. This process is repeated until we decide to reroute one unit of the bread sent from some bakery $x_N$ to $y_N$, back to $y_1$, at which point the cycle is complete.\\This new plan is cheaper than the old one, if and only if it holds
\[
c(x_1,y_2)+c(x_2,y_3)+\ldots+c(x_N,y_1) < c(x_1,y_1)+c(x_2,y_2)+\ldots+c(x_N,y_N).
\] 
If one can find a cycle of rerouting such that the upper inequality is satisfied, we know that our original transference plan can't be optimal, which brings us to the definition of cyclical monotonicity.
\subsection{Cyclical monotonicity}
\subsubsection{Definition}
Let $\mathcal{X,Y}$ be some sets and let $c:\mathcal{X\times{}Y}\rightarrow(-\infty,\infty]$ be a cost function. We call a subset $\Gamma\subset\mathcal{X\times{}Y}$ cyclically montone if, for any $N\in\mathbb{N}$ and any family $(x_1,y_1),\ldots,(x_N,y_N)\in\Gamma$ it holds
\[
\sum\limits_{i=1}^{N}c(x_i,y_i) \leq \sum\limits_{i=1}^{N}c(x_i,y_{i+1})\quad(\text{whereby }\, y_{N+1}=y_1).
\]
We call a transference plan $\pi$ between the two sets $\mathcal{X}$ and $\mathcal{Y}$ cyclically montone, if it is concentrated on some cyclically monotone subset of $\mathcal{X\times{}Y}$.\vspace{1em}\\
Intuitively this means, that a transportation plan is cyclically monotone, if it can not be improved by rerouting mass as described above.
It is obvious that an optimal transference plan is cyclically monotone, due to its optimality. What's interesting about this definition, is that any cyclically monotone transference plan is already optimal, which is a consequence of theorem 3.4.1.
\subsection{The dual Kantorovich problem}
Let's suppose now, that instead of doing the transportation ourselves, a logistics company steps in, buying bread from the bakeries and selling it to the respective cafés. Let $\psi(x)$ be the price that the company pays for one unit of bread at bakery $x$ and let $\phi(y)$ be the price at which it sells a unit of bread to café $y$. Instead of the original cost $c(x,y)$ the consortium now pays $\phi(y)-\psi(x)$ per unit of bread delivered from $x$ to $y$.
For the company to remain in business, it needs to set up prices such that 
\[
\phi(y)-\psi(x)\leq{}c(x,y),\,\,\quad(x,y)\in\mathcal{X\times{}Y},
\]
otherwise the consortium could just make the delivery cheaper themselves. We call such a pair of prices competitive. In this case it is the goal of the company to maximise their revenue, given by
\[
\int\limits_{\mathcal{Y}}\phi(y)d\nu(y) - \int\limits_{\mathcal{X}}\psi(x)d\mu(x)
\]
over all pairs of competitive prices. At this point we impose that the prices $\phi$ and $\psi$ are integrable. Later on this will follow from a bounding property of $c$ and the fact that the prices are competitive.\vspace{1em}\\
Given the set $\Pi(\mu,\nu)$ of all probability measures on $\mathcal{X\times{}Y}$ with marginals $\mu$ and $\nu$. With the help of the company, the transportation of each unit of bread is not more expensive than, when the consortium was doing it themselves. We therefore obtain
\[
\sup\limits_{\phi-\psi\leq{}c}\Bigg\lbrace\int\limits_{\mathcal{Y}}\phi(y)d\nu(y) - \int\limits_{\mathcal{X}}\psi(x)d\mu(x)\Bigg\rbrace \leq \inf\limits_{\pi\in\Pi(\mu,\nu)}\Bigg\lbrace\int\limits_{\mathcal{X\times{}Y}}c(x,y)d\pi(x,y)\Bigg\rbrace.
\]
It is, of course the company's goal to set the highest selling price $\phi$ and lowest buying price $\psi$ possible, while remaining competitive.\\
Consider an arbitrary pair of competitive prices $(\phi,\psi)$. Then, of course we can obtain a higher selling price $\phi_1(y) = \inf_{x\in\mathcal{X}}\left(\psi(x)+c(x,y)\right)$ while remaining competitive. $\psi$ can be improved by replacing it with $\psi_1(x) = \sup_{y\in\mathcal{Y}}\left(\phi_1(y)-c(x,y)\right)$.
Now take $\phi_2(y) = \inf_{x\in\mathcal{X}}\left(\psi_1(x)+c(x,y)\right)$ and $\psi_2(x) = \sup_{y\in\mathcal{Y}}\left(\phi_1(y)-c(x,y)\right)$. In every step of this sequence we obtain better buying and selling prices for the company, while remaining competitive. 
We prove that this process is stationary.\vspace{2em}\\
We know that $\phi_1\geq\phi$ and it is 
\begin{align*}
\phi_2(y) & = \inf_{x\in\mathcal{X}}\left(\psi_1(x)+c(x,y)\right) = \inf_{x\in\mathcal{X}}\left(\sup_{z\in\mathcal{Y}}\left(\phi_1(z)-c(x,z)\right)+c(x,y)\right) \\
& \geq \inf_{x\in\mathcal{X}}\left(\phi_1(y)-c(x,y)+c(x,y)\right)\newline = \phi_1(y).
\end{align*}
For $\psi_2$ we get
\begin{align*}
\psi_2(x) & = \sup_{y\in\mathcal{Y}}\left(\phi_2(y)-c(x,y)\right) \geq \sup_{y\in\mathcal{Y}}\left(\phi_1(y)-c(x,y)\right) = \psi_1(x), \\
\psi_2(x) & = \sup_{y\in\mathcal{Y}}\left(\phi_2(y)-c(x,y)\right) = \sup_{y\in\mathcal{Y}}\left(\inf_{z\in\mathcal{X}}\left(\psi_1(z)+c(z,y)\right)-c(x,y)\right) \\ & \leq \sup_{y\in\mathcal{Y}}\left(\psi_1(x)+c(x,y)-c(x,y)\right) = \psi_1(x).
\end{align*}
\noindent For $\psi_1$ it is
\begin{align*}
\psi_1(x) & = \sup_{y\in\mathcal{Y}}\left(\phi_1(y)-c(x,y)\right) \geq \sup_{y\in\mathcal{Y}}\left(\phi(y)-c(x,y)\right) = \psi(x), \\
\psi_1(x) & = \sup_{y\in\mathcal{Y}}\left(\phi_1(y)-c(x,y)\right) = \sup_{y\in\mathcal{Y}}\left(\inf_{z\in\mathcal{X}}\left(\psi(z)+c(z,y)\right)-c(x,y)\right) \\& \leq  \sup_{y\in\mathcal{Y}}\left(\psi(x)+c(x,y)-c(x,y)\right) = \psi(x).
\end{align*}
And for $\phi_2$
\begin{align*}
\phi_2(y) & = \inf_{x\in\mathcal{X}}\left(\psi_1(x)+c(x,y)\right) = \inf_{x\in\mathcal{X}}\left(\psi(x)+c(x,y)\right) = \phi_1(y).  
\end{align*}
\vspace{1em}\\
\noindent We therefore obtain $\phi_1=\phi_2 $ and $\psi_1=\psi_2$. This shows that, given an arbitrary pair of competitive prices one iteration of the process above will deliver a pair of prices that can not be improved without the company losing its competitivity. We call such a pair of prices $(\phi,\psi)$ tight. They then obviously satisfy. 
\[
\phi(y) = \inf_{x\in\mathcal{X}}\left(\psi(x)+c(x,y)\right),\qquad \psi(x) = \sup_{y\in\mathcal{Y}}\left(\phi(y)-c(x,y)\right)
\]
This means, when looking at competitive prices we can restrict our attention to $\psi$ and reconstruct $\phi$. Although taking an arbitrary buying price $\psi$ and reconstructing an optimal selling price $\phi$ doesn't guarantee that the pair is in fact tight. This brings us to the definition of c-convexity.
\subsection{C-convexity}
\subsubsection{Definition}
Given two sets $\mathcal{X},\mathcal{Y}$ and a function $c:\mathcal{X}\times\mathcal{Y}\rightarrow{}(-\infty,\infty]$. Then a function $\psi:\mathcal{X}\rightarrow{}\mathbb{R}\cup\{\pm\infty\}$ is called c-convex if it's not identically $+\infty$ and there exists $\phi$ such that  
\[
\forall x\in\mathcal{X}\quad \psi(x)=\sup_{y\in\mathcal{Y}}\left(\phi(y)-c(x,y)\right).
\]
Then its c-transform $\psi^c$ is defined by 
\[
\psi^c(y)=\inf_{x\in\mathcal{X}}\left(\psi(x)+c(x,y)\right),\quad \text{f.a. }y\in\mathcal{Y}.
\]
Since c-convexity depends on the cost function $c$, there are particular cases, in which a c-convex function in regard to a certain cost function $c$ has nice properties.\\
For instance, given a metric space $(\mathcal{X},d)$ and the cost function $d:\mathcal{X\times{}X}\rightarrow{}[0,\infty)$ then for a c-convex function on $\mathcal{X}$ it obviously needs to hold
\[
\forall x,y\in\mathcal{X}\quad\psi^c(y)-\psi(x)\leq{}d(x,y).
\] 
And therefore 
\[
\psi^c=\psi. 
\]
Then we know $\psi(x)-\psi(y)\leq{}d(x,y)$, for all $x,y\in\mathcal{X}$,\,\, i.e.\,\, $\psi$\,\, is Lipschitz continuous with Lipschitz constant 1.\vspace{1em}\\
Conversely, a 1-Lipschitz continuous function is c-convex with c-transform $\psi=\psi^c$, since
\[
\forall (x,y)\in\mathcal{X\times{}Y}\quad \psi(y)-\psi(x)\leq{}d(x,y). \quad\text{This means }\,\,\psi(y)=\inf\limits_{x\in\mathcal{X}}(\psi(x)+d(x,y)).
\vspace{1em}\\
\]
This now brings us to the Kantorovich duality, which will then lead us to the introduction of the Wasserstein metric.
\subsection{The Kantorovich duality}
\subsubsection{Theorem}
Let $(\mathcal{X},\mu)$ and $(\mathcal{Y},\nu)$ be two Polish probability spaces. Let $c:\mathcal{X\times{}Y}\rightarrow\mathbb{R}\cup\lbrace\infty\rbrace$ be a lower semicontinuous cost function such that there exist some upper semicontinuous, real-valued functions $a\in{}L^1(\mu)$ and $b\in{}L^1(\nu)$ satisfying
\[
\forall (x,y)\in\mathcal{X\times{}Y},\quad{}c(x,y)\geq{}a(x)+b(y).
\]
Then the following equality holds true
\begin{align*}
\min_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}Y}} c(x,y)d\pi(x,y) & = \sup_{\substack{(\psi,\phi)\in{}C_b(\mathcal{X})\times{}C_b(\mathcal{Y}) \\ \phi-\psi\leq{}c}} \left(\int\limits_{\mathcal{Y}} \phi(y)d\nu(y) - \int\limits_{\mathcal{X}}\psi(x)d\mu(x)\right) \\ & =\sup_{\substack{(\psi,\phi)\in{}L^1(\mu)\times{}L^1(\nu) \\ \phi-\psi\leq{}c}} \left(\int\limits_{\mathcal{Y}} \phi(y)d\nu(y) - \int\limits_{\mathcal{X}}\psi(x)d\mu(x)\right)   \\ &=\sup_{\psi\in{}L^1(\mu)}\left(\int\limits_{\mathcal{Y}} \psi^c(y)d\nu(y) - \int\limits_{\mathcal{X}}\psi(x)d\mu(x)\right).
\end{align*}
Here we can simply impose that $\psi$ is c-convex.\vspace{1em}\\
If the transport cost associated with the optimal coupling $\pi$ is finite, then there is a measurable cyclically monotone set $\Gamma\subset\mathcal{X\times{}Y}$ such that for $\pi\in\Pi(\mu,\nu)$ the following are equivalent:
\begin{center}\begin{tabular}{rl}
$(i)$& $\pi$ is optimal\\
$(ii)$& $\pi$ is cyclically monotone\\
$(iii)$& There is a c-convex function $\psi$ such that $\psi^c(y)-\psi(x)=\pi(x,y),\,\, \pi-$almost surely\\
$(iv)$& $\pi$ is concentrated on $\Gamma.$
\end{tabular}
\end{center} 
This is what was stated in section 3.1 (cyclical monotinicity).\vspace{1em}\\
If, in addition to the cost of the minimising transference plan being finite, there exist $c_{\mathcal{X}}\in{}L^1(\mu)$ and $c_{\mathcal{Y}}\in{}L^1(\nu)$, such that $c(x,y)\leq{}c_{\mathcal{X}}(x)+c_{\mathcal{Y}}(y)$, then the supremum on the right-hand side is attained and therefore a maximum. The minimum being attained on the left-hand side of the equation is a direct consequence of Theorem 2.4.1. This now brings us to the main focus of this bachelor thesis, the Wasserstein distance.
\section{The Wasserstein distance}
In the analogy above, we were given two probability measures $\mu, \nu$ and an arbitrary lower semicontinuous cost function $c$ and inspected the optimal transport cost, given by 
\[
C=\inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}Y}}c(x,y)d\pi(x,y).
\]
We now look at the same problem, but change the given cost function $c$ to $d^p$, whereby $d$ is a metric, inducing the $\sigma$-algebra on which the given probability measures are defined. This then introduces some notion of distance between the two probability measures, called the Wasserstein distance. Note here that $d$ of course satisfies the conditions of theorem 2.4.1, giving us a minimum on the left-hand side of the equation. It also satisfies the conditions of the dual Kantorovich theorem 3.4.1. We will use this later on.
\subsubsection{Definition}
Let $(\mathcal{X},d)$ be a Polish space and $p\in[1,\infty)$. For two probability measures $\mu$ and $\nu$ on the Borel $\sigma$-algebra on $\mathcal{X}$, induced by the given metric $d$, the Wasserstein distance of order $p$ between $\mu$ and $\nu$ is defined by
\begin{align*}
W_p(\mu,\nu) & = \inf_{\pi\in\Pi(\mu,\nu)}\left(\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y)\right)^{\frac{1}{p}} \\[12pt]& = \inf\bigg\lbrace\Big(\mathbb{E}\,d(X,Y)^p\Big)^{\frac{1}{p}}; \text{ law}(X) = \mu,\,\, \text{ law}(Y) = \nu\bigg\rbrace.
\end{align*}
This can now be interpreted as the minimum cost of transportation between the bakeries and the cafés, whereby the cost function is simply given by the distance between the respective production and consumption units. The Wasserstein distance is oftentimes also referred to as the earth mover's distance, in respect to the Monge problem. We briefly introduce a proposition to show what the Wasserstein distance may look like. 
\subsubsection{Proposition}
Take two arbitrary points $x,y\in\mathcal{X}$ and $a,b\in{}[0,1]$, let 
\[
\mu = a\delta_x+(1-a)\delta_y,\quad v=b\delta_x+(1-b)\delta_y
\]
be two probability measures on some measurable space $\mathcal{X}$, where the $\sigma$-algebra contains all sets with only one element. Then it holds $W_p(\mu,\nu) = |a-b|^\frac{1}{p}d(x,y)$.
\begin{proof}
Take a coupling $\pi$ of $(\mu,\nu)$, we know that $\pi$ is concentrated on $\lbrace{}(x,x),(x,y),(y,x),(y,y)\rbrace\subset\mathcal{X\times{}X}$. This yields
\begin{align*}
\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y) & = \left(\pi(x,y)+\pi(y,x)\right)d(x,y)^p.
\end{align*}
Due to the marginal property of $\pi$ we obtain 
\begin{align*}
&\pi(x,x)+\pi(y,x) = \nu(x) = b \\ &\pi(x,x)+\pi(x,y) = \mu(x) = a.
\end{align*}
For $a\leq{}b$ we know that $\pi(x,y)\leq\pi(y,x)$ and therefore the infimum is attained by setting $\pi(x,y)=0$, $\pi(x,x)=a$, $\pi(y,x)=b-a$, which implies $W_p(\mu,\nu)^p=(b-a)d(x,y)^p$. \vspace{1em}\\The proof for $b\leq{}a$ is analogous with $\pi(y,x)=0$, $\pi(x,x)=b$ and $\pi(x,y)=a-b.$
\end{proof}
\subsection{Axiomatic properties of the Wasserstein distance}
\noindent{}While $W_p$ isn't necessarily finite, due to the fact, that $(\mathcal{X},d)$ doesn't need to be bounded, we can show that the Wasserstein distance satisfies all other axioms of a metric on $\mathcal{P(X)}$.
We introduce a Lemma without proof for this.
\subsubsection{Gluing Lemma}
Let $(\mathcal{X}_i,\mu_i), i = 1, 2, 3,$ be Polish probability spaces. If $(X_1,X_2)$ is a coupling of $(\mu_1,\mu_2)$ and $(Y_2,Y_3)$ is a coupling of $(\mu_2,\mu_3)$, then one can construct a triple of random variables $(Z_1,Z_2,Z_3)$ such that $(Z_1,Z_2)$ has the same law as $(X_1,X_2)$ and $(Z_2,Z_3)$ has the same law as $(Y_2,Y_3).$
It is easy to see why it's called gluing Lemma. If the law of $(X_1,X_2)$ is given by $\pi_{12}$ and the law of $(X_2,X_3)$ by $\pi_{23}$ then we construct $\pi_{123}$ by simply "gluing" together $\pi_{12}$ and $\pi_{23}$ along their common marginal $\mu_2$.
\subsubsection{Theorem}
The Wasserstein distance $W_p$ satisfies all axioms of a metric, apart from finiteness.
\begin{proof}
We know that given a measurable space $(\Omega,\mathbb{P})$, if $\pi$ is a coupling of a tuple of probability measures $(\mu,\nu)$, i.e $\pi=\mathbb{P}\circ{}(X,Y)^{-1}$ for some random variables $X,Y$ then $\tilde{\pi}=\mathbb{P}\circ{}(Y,X)^{-1}$ is a coupling of $(\nu,\mu)$, which yields 
\begin{align*}
W_p(\mu,\nu)^p = \inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y) = \inf\limits_{\tilde{\pi}\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}X}}d(y,x)^pd\tilde{\pi}(y,x) = W_p(\nu,\mu)^p. 
\end{align*}
Since $d\geq{}0$ is a metric we already know that $W_p\geq{}0$. \vspace{1em}\\
For $(\mu,\nu)$ with $\mu=\nu$, we know that $\mu\circ{}(\text{Id},\text{Id})^{-1}$ is a coupling of $(\mu,\nu)$ with 
\[
\mu\circ(\text{Id},\text{Id})^{-1}(A\times{}B) = \mu(A\cap{}B).
\]
From this we conclude that the given coupling is concentrated on the diagonal and since the Wasserstein distance is based on the metric $d$ on $\mathcal{X}$ we obtain $W_p(\mu,\nu)=0$.\vspace{1em}\\
$W_p(\mu,\nu)=0$ implies there exists some coupling of $(\mu,\nu)$ given by $\mathbb{P}\circ{}(X,Y)^{-1}$, that's concentrated on the diagonal. We conclude $X = Y\,\mathbb{P}$-as. For any measurable $A\subset\mathcal{X}$ it then holds
\begin{align*}
\mu(A)=\mathbb{P}\circ(X,Y)^{-1}(A\times\mathcal{X}) = \mathbb{P}\circ(X,X)^{-1}(A\times\mathcal{X}) = \mathbb{P}\circ(X,X)^{-1}(\mathcal{X}\times{}A)=\nu(A),
\end{align*}
which implies $\mu=\nu.$\vspace{1em}\\
For the triangle inequality we use the gluing Lemma.\\
Let $\mu_1,\mu_2$ and $\mu_3$ be three probability measures on $\mathcal{X}$ and let $(X_1,X_2)$ be an optimal coupling of $(\mu_1,\mu_2)$ and $(Y_2,Y_3)$ an optimal coupling of $(\mu_2,\mu_3)$. Then there exist random variables $(Z_1,Z_2,Z_3)$ with law$(Z_1,Z_2)=\,\,$law$(X_1,X_2)$ and law$(Z_2,Z_3)=\,\,$law$(Y_2,Y_3)$. In particular $(Z_1,Z_3)$ is a coupling of $(\mu_1,\mu_3)$ and we obtain 
\begin{align*}
W_p(\mu_1,\mu_3) &\leq \left(\mathbb{E}\,d(Z_1,Z_3)^p \right)^\frac{1}{p} \\ & \leq \left(\mathbb{E}(d(Z_1,Z_2) + d(Z_3,Z_3))^p\right)^\frac{1}{p} \\ & \leq \left(\mathbb{E}\,d(Z_1,Z_2)^p\right)^\frac{1}{p} + \left(\mathbb{E}\,d(Z_2,Z_3)^p\right)^\frac{1}{p} \\ & = W_p(\mu_1,\mu_2) + W_p(\mu_2,\mu_3).
\end{align*} 
In this case the third inequality is based on the Minkowski inequality in $L^p(\mathbb{P})$.
\end{proof} 
\subsection{The Wasserstein space}
We now want $W_p$ additionally to the upper axioms to also be finite, in order for it to be useful. We reduce the space of all probability measures $\mathcal{P}(X)$ to a subspace on which $W_p$ is finite.
\subsubsection{Definition}
Given an arbitrary point $x_0\in\mathcal{X}$ we define
\[
P^{x_0}_p(\mathcal{X}):=\bigg\lbrace{}\mu\in\mathcal{P(X)};\quad \int\limits_{\mathcal{X}} d(x_0,x)^p d\mu(x)<\infty\bigg\rbrace.
\]
It is obvious that for $d$ bounded, it holds $P^{x_0}_p(\mathcal{X})=\mathcal{P(X)}$.\vspace{1em}\\We show that one can choose $x_0\in\mathcal{X}$ randomly, without changing $P^{x_0}_p(\mathcal{X})$.\\
Given $x_0,y_0\in\mathcal{X}$ arbitrary, it holds for $\mu\in{}P^{x_0}_p(\mathcal{X})$
\[
\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu(x)<\infty.
\]
With the triangle inequality for $d$ and Jensen's inequality we obtain\vspace{1em}\\
\begin{align*}
\int\limits_{\mathcal{X}}d(y_0,x)^pd\mu(x)&\leq\int\limits_{\mathcal{X}}(d(x_0,y_0)+d(x_0,x))^pd\mu(x)\\&\leq\int\limits_{\mathcal{X}}2^{p-1}d(x_0,y_0)^pd\mu(x) + \int\limits_{\mathcal{X}}2^{p-1}d(x_0,x)^pd\mu(x)\\&<\infty,
\end{align*}
\vspace{1em}\\
which means $\mu\in{}P^{y_0}_p(\mathcal{X})$. The inversed implication follows analogously. Therefore $P^{x_0}_p(\mathcal{X})=P_p(\mathcal{X})$ is independant of the choice of $x_0.$
\subsubsection{Theorem}
The Wasserstein distance $W_p$ is finite on the Wasserstein space $P_p(\mathcal{X})$ and therefore a metric.
\begin{proof}
We take two probability measures $\mu$ and $\nu$ in $P_p(\mathcal{X})$ and a coupling $\pi$ of the two. Once again, the triangle inequality for $d$ and Jensen's inequality yield\vspace{1em}\\
\begin{align*}
W_p(\mu,\nu) & \leq \left(\int\limits_{\mathcal{X\times{}X}}d(x,y)^p d\pi(x,y)\right)^\frac{1}{p} \leq \left(\int\limits_{\mathcal{X\times{}X}}(d(x,x_0)+d(x_0,y))^pd\pi(x,y)\right)^\frac{1}{p} \\ & \leq \left(\int\limits_{\mathcal{X\times{}X}}2^{p-1}\left(d(x,x_0)^p + d(x_0,y)^p\right)d\pi(x,y)\right)^\frac{1}{p} \\ & = \left(2^{p-1}\int\limits_{\mathcal{X}} d(x,x_0)^pd\mu(x) + 2^{p-1}\int\limits_{\mathcal{X}}d(x_0,y)^pd\nu(y)\right)^\frac{1}{p} < \infty.
\end{align*}
\end{proof}
\noindent{}Here is a useful consequence of the Kantorovich duality:\\
Seeing as a c-convex function in regard to the cost function $c=d$ is 1-Lipschitz and the fact that $d$ satisfies all conditions of the Kantorovich duality, we obtain
\[
W_1(\mu,\nu) = \sup\bigg\lbrace\int\limits_{\mathcal{X}}\psi(x)d\mu(x) - \int\limits_{\mathcal{X}}\psi(y)d\nu(y) \,;\hspace{0.5em} \psi \text{ is 1-Lipschitz}\bigg\rbrace.
\]
This is also known as the Kantorovich-Rubinstein formula.\vspace{1em}\\
For $p>1$ it holds 
\[
W_p(\mu,\nu)^p = \sup\Bigg\lbrace\int\limits_{\mathcal{X}}\psi^c(y)d\nu(y)-\int\limits_{\mathcal{X}}\psi(x)d\mu(x)\,;\hspace{0.5em} \psi\in{}C_b(\mathcal{X})\Bigg\rbrace,\vspace{1em}\\
\]
with $\psi^c(y)=\inf\limits_{x\in\mathcal{X}}\left(\psi(x)+d(x,y)^p\right)$.\vspace{2em}\\
The Wasserstein distance is a very popular distance on the space of probability measures and exhibits many nice properties. 
\begin{itemize}
\item[$(i)$]It is \,$W_p(\mu,\nu) = \inf\limits_{\pi\in\Pi(\mu,\nu)}||d||_{L^p(\pi)}$ and therefore Hölder's inequality implies that for $p\leq{}q$ it holds
\begin{align*}
W_p\leq{}W_q.
\end{align*}
\item[$(ii)$]The Wasserstein distance is defined by an infimum, which makes it fairly easy to bound from above. Looking at the Kantorovich-Rubinstein formula, we see it is defined as a supremum aswell, which makes it nice to bound from below. The construction of any coupling will bound it from above. Finding a pair of tight price-functions $\phi$ and $\phi^c$ will bound it from below. 
\item[$(iii)$]The mapping $x\mapsto\delta_x$ defines an isometry between $\mathcal{X}$ and $P_p(\mathcal{X})$, since any coupling of two dirac measures was already given by the tensor product of the two. Therefore it holds for $x_0,y_0\in\mathcal{X}$
\begin{align*}
W_p(\delta_{x_0},\delta_{y_0})^p = \inf\limits_{\pi\in\Pi(\delta_{x_0},\delta_{y_0})}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y) = \int\limits_{\mathcal{X}}\int\limits_{\mathcal{X}}d(x,y)^pd\delta_{x_0}d\delta_{y_0} = d(x_0,y_0)^p.
\end{align*}
\item[$(iv)$]For a C-Lipschitz $f:\mathcal{X}\rightarrow\mathcal{X'}$ the mapping $P_p(\mathcal{X})\rightarrow{}P_p(\mathcal{X'}),\mu\rightarrow{}\mu\circ{}f^{-1}$ is also C-Lipschitz, since
\begin{align*}
W_p(\mu\circ{}f^{-1},\nu\circ{}f^{-1})^p &= \inf\limits_{\pi\in\Pi(\mu\circ{}f^{-1},\nu\circ{}f^{-1})}\int\limits_{\mathcal{X'\times{}X'}}d(x,y)^pd\pi(x,y) \\&= \inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}X}}d(f(x),f(y))^pd\pi(x,y)\\&\leq{}\inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}X}}Cd(x,y)^pd\pi(x,y) \\&= C\cdot{}W_p(\mu,\nu)^p.
\end{align*}
\item[$(v)$] Given a probability space $(\Omega,\mathbb{P})$ and two random variables $X,Y:\Omega\rightarrow\mathbb{R}$ with $X,Y\in{}L^p(\mathbb{R})$ it is \begin{align*}
W_p(\mathbb{P}\circ{}X^{-1},\mathbb{P}\circ{}Y^{-1})\leq\|X-Y\|_{p},
\end{align*} 
since $(X,Y)$ is a coupling of $\mathbb{P}\circ{}X^{-1}$ and $\mathbb{P}\circ{}Y^{-1}.$
Therefore the mapping $L^p(\mathbb{R})\rightarrow{}P_p(\mathbb{R})$, that projects a random variable onto its law is a contraction. Recall theorem 2.2.3, stating that a coupling of a dirac measure and any other probability measure, is given by the product measure, then for a random variable $X$ on the probability space $(\Omega,\mathbb{P})$, it holds
\begin{align*}
W_p(\mathbb{P}\circ{}X^{-1},\delta_0) = \|X\|_{p}.
\end{align*}
\item[$(vi)$] There exists a continuous bijection between the Wasserstein space $L^p(\mathbb{R})$ and $P_p(\mathbb{R})$. This follows from the fact, that for any probability measure there exists a random variable, with the same law.
\end{itemize}
\section{Topological properties of the Wasserstein distance}
We have come up with the Wasserstein space $P_p(\mathcal{X})\subset\mathcal{P(X)}$, which we now want to endow with a topology, allowing the concept of openness and closedness on $P_p(\mathcal{X})$. 
\subsection{Weak convergence on the Wasserstein space}
Since the Wasserstein space was a subspace of the space of all probability measures, the topology describing convergence needs to be at least as great as the topology of weak convergence, while additionally retaining the finiteness of the integral, so that the limit remains in $P_p(\mathcal{X})$. We define convergence on the Wasserstein space as follows.
\subsubsection{Definition}
Let $(\mathcal{X},d)$ be a Polish space and $p\in\left[1,\infty\right)$. Let $(\mu_n)_{n\in\mathbb{N}} \subset P_p(\mathcal{X})$ be a sequence of probability measures and $\mu\in{}P_p(\mathcal{X})$. We then say that $(\mu_n)_{n\in\mathbb{N}}$ converges weakly to $\mu$ in $P_p(\mathcal{X})$, if it converges weakly in terms of regular weak convergence and for any arbitrary $x_0\in\mathcal{X}$ any of the following equivalent properties is satisfied\\
\begin{align*}
(i)&\hspace{0.5em}\lim\limits_{k\rightarrow\infty}\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu_k(x)=\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu(x) \\[10pt]
(ii)&\hspace{0.5em}\limsup\limits_{k\rightarrow\infty}\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu_k(x)\leq\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu(x)\\[8pt]
(iii)&\hspace{0.5em}\lim\limits_{R\rightarrow\infty}\limsup\limits_{k\rightarrow\infty}\int\limits_{d(x_0,x)\geq{}R}d(x_0,x)^pd\mu_k(x)=0.
\end{align*}  
This clearly shows that, if $d$ isn't bounded, weak convergence on $P_p(\mathcal{X})$ is stronger than regular weak convergence, due to the additional conditions, posted above.\vspace{1em}\\For instance, if $d$ isn't bounded, one can consider the sequence of probability measures on $P_p(\mathcal{X})$ given by a sequence and a point $(x_n)_{n\in\mathbb{N}}, x_0$ in $\mathcal{X}$, with $r_n=d(x_0,x_n)\rightarrow\infty$ and
\begin{align*}
\mu_n = (1-r^{-p}_n)\,\delta_{x_0} +\, r^{-p}_n\,\delta_{x_n}.
\end{align*}
This sequence obviously converges weakly with weak limit $\mu=\delta_{x_0}$, but it also holds
\begin{align*}
W_p(\mu_n,\mu) = |1-r^{-p}_n\,-\,1|^{\frac{1}{p}}\,d(x_n,x_0) = 1
\end{align*}
for all $n\in\mathbb{N}$ as was shown in proposition 4.0.2. Therefore a weakly convergent sequence in $\mathcal{P(X)}$ isn't necessarily weakly convergent in $P_p(\mathcal{X})$.\vspace{1em}\\
For $d$ bounded we know that weak convergence on $P_p(\mathcal{X})$ is equivalent to regular weak convergence, because condition $(iii)$ of 5.1 is automatically satisfied for any weakly convergent sequence in $\mathcal{P(X)}$. We then of course also know that $P_p(\mathcal{X})=\mathcal{P(X)}.$\vspace{1em}\\
Before we go on to proving that the Wasserstein distance metrizes weak convergence on $P_p(\mathcal{X})$ we provide a few lemmas.
\subsubsection{Lemma}
Let $(\mathcal{X},d)$ be a Polish space, $p\in[1,\infty)$. Then every Cauchy sequence in $(P_p(\mathcal{X}),W_p)$ is tight.
\begin{proof}
Take a Cauchy sequence $(\mu_n)_{n\in\mathbb{N}}\subset{}P_p(\mathcal{X})$, then
\begin{align*}
W_p(\mu_k,\mu_l)\rightarrow{}0\quad\quad\quad\quad{}(l,k\rightarrow\infty).
\end{align*} 
We know that for every $\varepsilon>0$ there exists $N\in\mathbb{N}$ such that for all $k\geq{}N$ it holds
\begin{align*}
W_p(\mu_N,\mu_k)\leq\varepsilon^2.
\end{align*}
And given $W_1\leq{}W_p$ for $p\geq{}1$, we obtain $W_1(\mu_N,\mu_k)\leq\varepsilon^2$ for all $k\geq{}N$. \vspace{1em}\\Now it is obvious that for every $k\in\mathbb{N}$, there exists $j\in\{1,\ldots,N\}$ such that $W_1(\mu_k,\mu_j)\leq\varepsilon^2$ (For $k\leq{}N$ choose $j=k$ and for $k\geq{}N$ choose $j=N$).\vspace{1em}\\
Since we were given a Polish space we know that every probability measure on $\mathcal{X}$ is tight and therefore every finite set of probability measures is tight, because the finite union of compact sets is compact. Hence we can find a compact $K\subset\mathcal{X}$ such that  
\begin{align*}
\sup\limits_{k\leq{}N} \mu_k(K^c)\leq\varepsilon.
\end{align*}
By compactness, $K$ can be covered in finitely many balls of radius $\varepsilon$, 
\begin{align*}
K\subset U:= B(x_1,\varepsilon)\cup\ldots\cup{}B(x_m,\varepsilon) \,\,\,\text{ with }\, x_1,\ldots,x_m\in\mathcal{X}.
\end{align*}
This means for all $k\leq{}N$ it is $\mu_k(U^c)\leq\varepsilon$. We now use the fact that for all $k\in\mathbb{N}$ it holds $W_1(\mu_N,\mu_k)\leq\varepsilon^2$, to show that there exists a measurable set $U_\varepsilon\supset{}U$ with $\sup\limits_{k\in\mathbb{N}}\mu({U_\varepsilon}^c)\leq{}2\varepsilon$.
Define 
\begin{align*}U_\varepsilon:=\Big\lbrace{}x\in\mathcal{X}; d(x,U)<\varepsilon\Big\rbrace \subset B(x_1,2\varepsilon)\cup\ldots\cup B(x_m,2\varepsilon)
\end{align*}
and
\begin{align*}
\phi_\varepsilon(x):=\left(1-\frac{d(x,U)}{\varepsilon}\right)_{+}.
\end{align*}
Obviously it is $1_U\leq\phi_\varepsilon\leq{}1_{U_\varepsilon}.$ We show that $\phi_\varepsilon$ is Lipschitz continuous with Lipschitz constant $\frac{1}{\varepsilon}.$ For all $x,y\in\mathcal{X}$ with $\phi_\varepsilon(x)\geq\phi_\varepsilon(y)$ it holds 
\begin{align*}
|\phi_\varepsilon(x)-\phi_\varepsilon(y)| & = \left(1-\frac{d(x,U)}{\varepsilon}\right)_{+}- \hspace{0.6em}\left(1-\frac{d(y,U)}{\varepsilon}\right)_{+} \\ &\leq \left(\frac{d(y,U)-d(x,U)}{\varepsilon}\right)_{+} \\&\leq \left(\frac{d(x,y)}{\varepsilon}\right)_{+} \\ &= \frac{d(x,y)}{\varepsilon}.
\end{align*}
For $x,y\in\mathcal{X}$ with $\phi_\varepsilon(y)\geq\phi_\varepsilon(x)$ the same inequality follows analogously.\\
Note that the Kantorovich duality was true for any 1-Lipschitz function $\phi$. We know that $\phi_\varepsilon$ is Lipschitz-continuous with constant $\frac{1}{\varepsilon}$ and therefore $\varepsilon\phi_\varepsilon$ is 1-Lipschitz and we can use theorem 3.4.1. We also know that given an arbitrary $k\in\mathbb{N}$ there exists $j\in\{1,\ldots,N\}$ such that $W_1(\mu_k,\mu_j)<\varepsilon^2$ and $\mu_j(U)\geq{}1-\varepsilon.$ \vspace{1em}\\This yields
\begin{align*}
\mu_k(U_\varepsilon) & = \int\limits_{\mathcal{X}}1_{U_\varepsilon}d\mu_k \\&\geq \int\limits_{\mathcal{X}}\phi_{\varepsilon}(x)d\mu_k \\& \geq\int\limits_{\mathcal{X}}\phi_\varepsilon(x)d\mu_j - \left(\int\limits_{\mathcal{X}}\phi_\varepsilon(x)d\mu_j - \int\limits_{\mathcal{X}}\phi_\varepsilon(y)d\mu_k\right) \\& \geq\int\limits_{\mathcal{X}}\phi_\varepsilon(x)d\mu_j - \frac{W_1(\mu_k,\mu_j)}{\varepsilon} \\& \geq\mu_j(U)-\frac{W_1(\mu_k,\mu_j)}{\varepsilon}.
\end{align*}
At this point we know that for every $\varepsilon>0$ we can find a finite union of balls $U_\varepsilon\subset\mathcal{X}$ such that for all $k\in\mathbb{N}$ it is 
\begin{align*}
\mu_k(U_\varepsilon)\geq{}1-\varepsilon-\frac{\varepsilon^2}{\varepsilon} = 1-2\varepsilon.
\end{align*}
We now face the problem that the set $U_\varepsilon$ isn't necessarily compact for every $\varepsilon>0$. Recall that $(\mathcal{X},d)$ is a Polish space. This means that for every $k\in\mathbb{N}$ there exists a finite family $\{x_1,\ldots,x_{m(l)}\}\subset\mathcal{X}$ such that it holds
\begin{align*}
\mu_k\left(\mathcal{X}\Big\backslash\bigcup\limits_{1\leq{}i\leq{}m(l)}B(x_i,2^{-l+1}\varepsilon{})\right)\leq{}2^{-l}\varepsilon.
\vspace{0.25em}
\end{align*} 
Now define $S:=\bigcap\limits_{1\leq{}l<\infty}\bigcup\limits_{1\leq{}i\leq{}m(l)} \overline{B(x_i,2^{-l+1}\varepsilon)}$ then for $k\in\mathbb{N}$ arbitrary it is
\begin{align*}
\mu_k(\mathcal{X}\backslash{}S) &= \mu_k\left(\bigcup\limits_{l\in\mathbb{N}}\left(\bigcup\limits_{i=1}^{m(l)}\overline{B(x_i,2^{-l+1}\varepsilon)}\right)^c\,\,\right)\\[5pt]&\leq\sum\limits_{l\in\mathbb{N}}\mu_k\left(\left(\bigcup\limits_{i=1}^{m(l)}\overline{B(x_i,2^{-l+1}\varepsilon)}\right)^c\,\,\right)\\[5pt]&\leq\sum\limits_{l\in\mathbb{N}}2^{-l}\varepsilon\\&=\varepsilon.
\end{align*}
Furthermore it is clear that $S$ is totally bounded and closed. Note that this implies compactness of $S.$
To recap, we now know that for every $\varepsilon$ there exists a compact $K\subset\mathcal{X}$ such that 
\[
\sup\limits_{k\in\mathbb{N}}\mu_k(K^c)\leq\varepsilon
\]
and therefore $(\mu_n)_{n\in\mathbb{N}}$ is tight.
\end{proof}
\subsubsection{Lemma}
For every $\varepsilon\geq{}0$ there exists a constant $C_\varepsilon$ such that for all $a,b\in\mathbb{R_+}$ it holds
\begin{align*}
(a+b)^p \leq (1+\varepsilon)a^p + C_\varepsilon{}b^p.
\end{align*} 
The proof to this requires a differentiation of cases.
\begin{proof}
By dividing by $a^p$ we see that the upper ineqaulity is equivalent to 
\begin{align*}
(1+x)^p\leq{}1+\varepsilon+C_{\varepsilon}x^p, \,\,\text{where } \hspace{0.3em}x=\frac{b}{a}.
\end{align*}
Case 1: $x\leq{}x_{\varepsilon}:=(1+\varepsilon)^{\frac{1}{p}}$
\begin{align*}
(1+x)^p \leq x^p+C_{\varepsilon}x^p \leq 1+\varepsilon + C_{\varepsilon}x^p.
\end{align*}
Case 2: $x>x_{\varepsilon}:=(1+\varepsilon)^{\frac{1}{p}}$\vspace{0.45em}\\
Choosing $C_{\varepsilon}:=(1+x_{\varepsilon}^{-1})^p$ yields
\begin{align*}
1+\varepsilon + C_{\varepsilon}x^p = 1+\varepsilon + (1+x_{\varepsilon}^{-1})^px^p \geq 1+\varepsilon+(1+x)^p\geq{}(1+x)^p.
\end{align*}
\end{proof}
\subsection{Prokhorov's theorem}
Let $(\mathcal{X},d)$ be a separable metric space and $\mathcal{P(X)}$ denote the space of probability measures on the Borel $\sigma$-algebra $\mathcal{B(X)}$ induced by $d$.
\begin{itemize}
\item[$(i)$] A subset $K\subset{}\mathcal{P(X)}$ is tight if and only if $\overline{K}$ is sequentially compact in the space equipped with the topology of weak convergence, i.e every infinite sequence $(\mu_k)_{k\in\mathcal{X}}\subset{}K$ has a subsequence that converges weakly.
\item[$(ii)$] The space $\mathcal{P(X)}$ endowed with the topology of weak convergence is metrizable.
\item[$(iii)$] If $(\mathcal{X},d)$ is a Polish space, there is a complete topology on $\mathcal{P(X)}$ equivalent to the topology of weak convergence.
\end{itemize}
\subsubsection{Lemma}
Let $(\mathcal{X},d)$ and $(\mathcal{Y},d')$ be two Polish spaces. Given two tight sets of probability measures $\mathcal{P}\subset{}\mathcal{P(X)}$ and $\mathcal{Q}\subset\mathcal{P(Y)}$, let $\Pi(\mathcal{P},\mathcal{Q})$ be the set of all couplings $\pi\in\mathcal{P(X\times{}Y)}$ between probability measures in $\mathcal{P}$ and $\mathcal{Q}$. Then $\Pi(\mathcal{P},\mathcal{Q})$ is tight as well.
\begin{proof}
Take an arbitrary $\varepsilon>0$. Knowing that $\mathcal{P}$ and $\mathcal{Q}$ are tight, we know that there are two compact sets $K\subset\mathcal{X}$ and $\tilde{K}\subset\mathcal{Y}$ such that 
\begin{align*}
\sup\limits_{\mu\in\mathcal{P}}\mu(K^c)<\varepsilon \hspace{1.5em}\text{and}\hspace{1.5em} \sup\limits_{\nu\in\mathcal{Q}}\nu(\tilde{K}^c)<\varepsilon.
\end{align*}
Note that for $K\subset\mathcal{X}$ and $\tilde{K}\subset\mathcal{Y}$ compact, $K\times\tilde{K}$ is compact in $\mathcal{X\times{}Y}$.\\
From this we conclude
\begin{align*}
&\sup\limits_{{\small \pi\in\Pi(\mathcal{P},\mathcal{Q})}}\pi\left((K\times\tilde{K})^c\right) \,\leq \sup\limits_{\pi\in\Pi(\mathcal{P},\mathcal{Q})}\pi(K^c\times\mathcal{Y}) \,\,\,+ \sup\limits_{\pi\in\Pi(\mathcal{P},\mathcal{Q})}\pi(\mathcal{X}\times\tilde{K}^c) \\[5pt]&= \sup\limits_{\mu\in\mathcal{P}}\mu(K^c) + \sup\limits_{\nu\in\mathcal{Q}}\nu(\tilde{K}^c) \\[5pt]&< 2\varepsilon.
\end{align*}
Therefore $\Pi(\mathcal{P},\mathcal{Q})$ is tight.
\end{proof}
\subsubsection{Lemma}
Let $\mathcal{X}$ and $\mathcal{Y}$ be two Polish spaces, $c:\mathcal{X\times{}Y}\rightarrow\mathbb{R}\cup\{+\infty\}$ a lower semicontinuous cost function and $h:\mathcal{X\times{}Y}\rightarrow\mathbb{R}\cup\{-\infty\}$ an upper semicontinuous function with $h\leq{}c.$\vspace{0.5em}\\ Given a sequence of probability measures $(\pi_k)_{k\in\mathbb{N}}$ on $\mathcal{X\times{}Y}$, that converges weakly to some $\pi\in\mathcal{P(X\times{}Y)}$, in such a way that $h\in{}L^1(\pi_k)$ for all $k\in\mathbb{N}$ and $h\in{}L^1(\pi)$.\vspace{1em}\\ If, additionally it holds
\[
\lim\limits_{k\rightarrow\infty}\int\limits_{\mathcal{X\times{}Y}}h(x,y)d\pi_k \quad = \quad \int\limits_{\mathcal{X\times{}Y}}h(x,y)d\pi,
\]
then
\[
\int\limits_{\mathcal{X\times{}Y}}c(x,y)d\pi \quad\leq\quad \liminf\limits_{k\rightarrow\infty}\int\limits_{\mathcal{X\times{}Y}}c(x,y)d\pi_k.
\]
\subsubsection{Lemma}
Let $(\mathcal{X},\mathcal{T})$ be a sequentially compact topological space and $(x_n)_{n\in\mathbb{N}}\subset{}\mathcal{X}$ a sequence, such that every convergent subsequence $(x_{n_k})_{k\in\mathbb{N}}$ has the same limit $x\in\mathcal{X}$.\\Then $(x_n)_{n\in\mathbb{N}}$ converges to $x$.
\begin{proof}
We presume $(x_n)_{n\in\mathbb{N}}$ doesn't converge to $x$, then there exists a neighbourhood $A\in\mathcal{T}$ of $x$ such that for every $n_0\in\mathbb{N}$ we can find $n\in\mathbb{N}$ with $n\geq{}n_0$ and $x_n\not\in{}A$.\vspace{1em}\\ We define a subsequence $(x_{n_k})_{k\in\mathbb{N}}$ such that for all $k\in\mathbb{N}$, $x_{n_k}\not\in{}A$, which due to the sequential compactness has a convergent subsequence $(x_{n_{k'}})_{k'\in\mathbb{N}}$ with limit x.
This, of course is a contradiction to our assumption: $x_{n_k}\not\in{}A$ for all $k\in\mathbb{N}$.\vspace{1em}\\
Consequently we conclude that no such sequence can exist, which means $x_n\rightarrow x.$
\end{proof}
\subsection{Metrization of weak convergence on the Wasserstein space}
We recall that we had defined a convergence on the Wasserstein space. This means we have endowed $P_p(\mathcal{X})$ with a topology. Since $W_p$ is a metric on $P_p(\mathcal{X})$ it also induces a topology on the Wasserstein space. It turns out, that these two are identical.
\subsubsection{Theorem}
Let $(\mathcal{X},d)$ be a Polish space and $p\in[1,\infty)$. The Wasserstein metric induces the topology of weak convergence on the Wasserstein space $P_p(\mathcal{X})$. This means that for $(\mu_k)_{k\in\mathbb{N}}\subset{}P_p(\mathcal{X})$ and $\mu\in{}P_p(\mathcal{X})$ the two following statements are equivalent:
\begin{align*}
(i)& \quad \mu_k \text{ converges weakly in } P_p(\mathcal{X}) \text{ to } \mu \\[5pt]
(ii)& \quad W_p(\mu_k,\mu)\rightarrow{}0.
\end{align*} 
\begin{proof}We start off by showing, that $(ii)$ implies $(i)$.\\
Given $(\mu_k)_{k\in\mathbb{N}}\subset{}P_p(\mathcal{X})$ and $\mu\in{}P_p(\mathcal{X})$ with $W_p(\mu_k,\mu)\rightarrow{}0$, we show that $(\mu_k)_{k\in\mathbb{N}}$ converges to $\mu$ in terms of regular weak convergence and additionally satisfies condition $(ii)$ of definition 5.1.1.\vspace{1em}\\
Due to convergence we know that $(\mu_k)_{k\in\mathbb{N}}$ is a Cauchy sequence in $(P_p(\mathcal{X}),W_p)$ and by Lemma 5.1.2, tight. By Prokhorov's theorem we know that $\{\mu_k|k\in\mathbb{N}\}$ is relatively sequentially compact. This means there exists a weakly convergent subsequence $(\mu_{k'})_{k'\in\mathbb{N}}$ of $(\mu_k)_{k\in\mathbb{N}}$ with some weak limit $\tilde{\mu}\in{}\mathcal{P(X)}$. Of course, it still holds $W_p(\mu_{k'},\mu)\rightarrow{}0$. \vspace{1em}\\
Since $(\mathcal{X},d)$ is a Polish space we know that every probability measure is tight, which means $\{\tilde{\mu}\}$ is tight. The sequence 
$(\mu_{k'})_{k'\in\mathbb{N}} $ is also tight, as a subsequence of a tight sequence. \\ We know by Lemma 5.2.1 that the set $\Pi(\{\mu_{k'}|k'\in\mathbb{N}\},\{\tilde{\mu}\})$ of all couplings between measures $\mu_{k'}$ and $\tilde{\mu}$ is tight as well, which, again by Prokhorov's theorem means that it is relatively sequentially compact. \vspace{1em}\\
Now, let $(\pi_{k'})_{k'\in\mathbb{N}}$ denote the sequence of optimal couplings of $(\mu_{k'},\mu)$.
We know that there exists a convergent subsequence of $(\pi_{k'})_{k'\in\mathbb{N}}$, which we will simply denote $(\pi_{k'})_{k'\in\mathbb{N}}$ again. Let $\tilde{\pi}\in{}P(\mathcal{X\times{}\mathcal{X}})$ be the weak limit of said subsequence, then we know that $\tilde{\pi}$ is a coupling of $(\tilde{\mu},\mu)$, because for every $f\in{}C_b(\mathcal{X})$ it holds 
\[
\int\limits_{\mathcal{X\times{}X}}f(x)d\tilde{\pi}(x,y) = \lim\limits_{{k'}\rightarrow{}\infty}\int\limits_{\mathcal{X\times{}X}}f(x)d\tilde{\pi}_{k'}(x,y) = \lim\limits_{{k'}\rightarrow{}\infty}\int\limits_{\mathcal{X}}f(x)d\tilde{\mu}_{k'}(x) = \int\limits_{\mathcal{X}}f(x)d\tilde{\mu}(x)
\] 
and of course
\[
\int\limits_{\mathcal{X\times{}X}}f(y)d\tilde{\pi}_{k'}(x,y) = \lim\limits_{{k'}\rightarrow{}\infty}\int\limits_{\mathcal{X\times{}X}}f(y)d\tilde{\pi}_{k'}(x,y) = \int\limits_{\mathcal{X}}f(y)d\mu(y).
\]
Hence $\tilde{\pi}$ satisfies the marginal conditions and therefore is a coupling of $(\tilde{\mu},\mu)$.\\ This yields 
\[
W_p(\tilde{\mu},\mu)^p = \inf\limits_{\pi\in\Pi(\tilde{\mu},\mu)}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y) \leq \int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\tilde{\pi}(x,y).
\]
Knowing that $(\pi_{k'})_{k'\in\mathbb{N}}$ converges weakly to $\tilde{\pi}$ we can use Lemma 5.2.2, setting $c(x,y)=d^p(x,y)$, which is obviously continuous as a composition of continuous functions, to obtain
\[
\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\tilde{\pi}(x,y) \leq \liminf_{k'\rightarrow{}\infty}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_{k'}(x,y) = \liminf_{k'\rightarrow{}\infty}W_p(\mu_{k'},\mu)^p = 0.
\]
This implies $W_p(\tilde{\mu},\mu)=0$, so $\tilde{\mu} = \mu$. We now know that $\{\mu_k\,|\,k\in\mathbb{N}\}$ is a relatively sequentially compact sequence whereby every convergent subsequence has the same limit. Then the closure is sequentially compact an by Lemma 5.2.3, we conclude that $(\mu_k)_{k\in\mathbb{N}}$ converges weakly to $\mu$. \vspace{2em}\\
At this point we have only shown that $W_p(\mu_k,\mu)\rightarrow{}0$ implies $\mu_k\rightarrow{}\mu$ in terms of regular weak convergence in $\mathcal{P(X)}$. We now need to prove that $(\mu_k)_{k\in\mathbb{N}}$ converges to $\mu$ in $P_p(\mathcal{X})$, by showing that for an arbitrary $x_0\in\mathcal{X}$ it holds
\[
\limsup\limits_{k\rightarrow{}\infty}\int\limits_{\mathcal{X}}d(x,x_0)^pd\mu_k(x) \leq{}\int\limits_{X}d(x,x_0)^pd\mu(x).
\]
Like before, let $(\pi_k)_{k\in\mathbb{N}}$ be the sequence of optimal couplings of $(\mu_k,\mu)$\\
We know by Lemma 5.1.3 that for every $\varepsilon\geq{}0$ there exists $C_{\varepsilon}\geq{}0$ such that for all $a,b\geq{}0$ it is 
\[
(a+b)^p\leq{}(1+\varepsilon)a^p + C_{\varepsilon}b^p.
\]
We apply this inequality to $d(x_0,x)$ for $x_0\in\mathcal{X}$ and obtain for arbitrary $\varepsilon\geq{}0$ \vspace{0.5em}\\
\[
d(x,x_0)^p\leq{}\left(d(x_0,y)+d(x,y)\right)^p \leq (1+\varepsilon)d(x_0,y)^p+C_{\varepsilon}d(x,y)^p.
\vspace{1em}
\] 
Integrating over $\pi_k$ yields
\[
\int\limits_{\mathcal{X\times{}X}}d(x,x_0)^pd\pi_k(x,y) \leq \int\limits_{\mathcal{X\times{}X}}(1+\varepsilon)d(x_0,y)^p + C_{\varepsilon}d(x,y)^pd\pi_k(x,y)
\]
and the marginal property of $\pi_k$ yields
\[
\int\limits_{\mathcal{X}}d(x,x_0)^pd\mu_k(x) \leq (1+\varepsilon)\int\limits_{\mathcal{X}}d(x_0,y)^pd\mu(y) + C_{\varepsilon}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_k(x,y).
\]
We know that $W_p(\mu_k,\mu)\rightarrow{}0$, which means 
\[
\limsup\limits_{k\rightarrow{}\infty}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_k(x,y) = 0.
\]
Therefore
\[
\limsup\limits_{k\rightarrow\infty}\int\limits_{\mathcal{X}}d(x,x_0)^pd\mu_k(x) \leq(1+\varepsilon)\int\limits_{\mathcal{X}}d(x_0,y)^pd\mu(y).
\]
Since this equality holds true for any $\varepsilon\geq{}0$ it is \vspace{1em}
\[
\limsup\limits_{k\rightarrow{}0}\int\limits_{\mathcal{X}}d(x,x_0)^pd\mu_k(x) \leq\int\limits_{\mathcal{X}}d(x,x_0)^pd\mu(x).
\]
Hereby we have shown property $(ii)$ in definition 5.1.1 of weak convergence in $P_p(\mathcal{X})$. This shows that $W_p(\mu_k,\mu)\rightarrow{}0$ implies $\mu_k\rightarrow{}\mu$ in $P_p(\mathcal{X})$. In other words the topology on $P_p(\mathcal{X})$ induced by $W_p$ is greater than the topology of weak convergence on $P_p(\mathcal{X})$.\vspace{1em}\\
We now show that $(i)$ implies $(ii)$. \\
Let $(\mu_k)_{k\in\mathbb{N}}\in{}P_p(\mathcal{X})$ be weakly convergent in $P_p(\mathcal{X})$ with limit $\mu\in{}P_p(\mathcal{X})$. It immediately follows that the sequence then also converges weakly in terms of regular weak convergence. By Prokhorov's theorem, the sequence is tight and so is $\{\mu\}$, since we're on a Polish space.\vspace{1em}\\ Let $(\pi_k)_{k\in\mathbb{N}}$ be the sequence of optimal couplings of $(\mu_k,\mu)$. Then, by Lemma 5.2.1 $(\pi_k)_{k\in\mathbb{N}}$ is tight and by Prokhorov's theorem there exists a weakly convergent subsequence $(\pi_{k'})_{k'\in\mathbb{N}}$, with some limit $\pi\in{}P(\mathcal{X\times{}X})$.\vspace{1em}\\
We know that $\pi$ is in fact a coupling of $(\mu,\mu)$, because for arbitrary $f\in{}C_b(\mathcal{X})$, $\pi$ satisfies the marginal conditions
\begin{align*}
&\int\limits_{\mathcal{X\times{}X}}f(x)d\pi(x,y) = \lim\limits_{k'\rightarrow\infty}\int\limits_{\mathcal{X\times{}X}}f(x)d\pi_{k'}(x,y) = \lim\limits_{k'\rightarrow\infty}\int\limits_{\mathcal{X}}f(x)d\mu_{k'}(x) = \int\limits_{\mathcal{X}}f(x)d\mu(x)
\\
&\int\limits_{\mathcal{X\times{}X}}f(y)d\pi(x,y) = \lim\limits_{k'\rightarrow\infty}\int\limits_{\mathcal{X\times{}X}}f(y)d\pi_{k'}(x,y) = \int\limits_{\mathcal{X}}f(y)d\mu(y).
\end{align*}
Since $(\pi_k')_{k'\in\mathbb{N}}$ converges weakly to $\pi$ we know by lemma 5.2.2 that
\[
\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y) \leq \liminf\limits_{k'\rightarrow\infty}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_{k'}(x,y).
\]
Therefore $\pi$ is an optimal coupling of $(\mu,\mu)$. If $\pi$ wasn't optimal there would be another coupling $\tilde{\pi}$ with 
\[
\int\limits_{\mathcal{X\times{}X}} d(x,y)^pd\tilde{\pi}(x,y) \leq \int\limits_{\mathcal{X\times{}X}} d(x,y)^pd\pi(x,y).
\]
Hence one could find another sequence $(\tilde{\pi}_{k'})_{k'\in\mathbb{N}}\subset{}P(\mathcal{X\times{}X})$ such that $\tilde{\pi}_{k'}\,\, \substack{\omega\\ \longrightarrow}\,\, \tilde{\pi}$.
But, of course by 5.2.2 it would follow  
\begin{align*}
\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y) &\leq \liminf\limits_{k'\rightarrow\infty}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_{k'}(x,y) \\& \leq \liminf\limits_{k'\rightarrow\infty}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\tilde{\pi}_{k'}(x,y) \\& \leq \int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\tilde{\pi}(x,y).
\end{align*}
This means $\tilde{\pi}=\pi$, i.e $\pi$ is already an optimal coupling of $(\mu,\mu)$ and since $W_p(\mu,\mu)=0$ it is 
\[
\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y)=0.
\]
In other words $\pi$ is concentrated on the diagonal $\{(x,x)|x\in\mathcal{X}\}$, such that $\pi=\mu\circ{}(\text{Id,Id})^{-1}.$\vspace{1em}\\
Since every weakly convergent subsequence of $(\pi_k)_{k\in\mathbb{N}}$ has the same limit $\pi$ and $(\pi_k)_{k\in\mathbb{N}}\cup\{\pi\}$ is sequentially compact, by Lemma 5.2.3 we know that the entire sequence converges weakly to $\pi$. \vspace{1em}\\
So far we have shown there exists a sequence of optimal couplings $(\pi_k)_{k\in\mathbb{N}}$ of $(\mu_k,\mu)$, that converges weakly to an optimal coupling $\pi$ of $(\mu,\mu)$. Using this and property $(iii)$ of weak convergence on $P_p(\mathcal{X})$ we will prove $\limsup\limits_{k\rightarrow\infty}W_p(\mu_k,\mu)=0.$ \vspace{1em}\\
Given an arbitrary $x_0\in\mathcal{X}$ and $R>0$, then $d(x,y)\geq{}R$ implies either 
\[
d(x,x_0)\geq\frac{R}{2} \hspace{1em}\text{ and }\hspace{1em} d(x,x_0)\geq{}d(y,x_0)
\]
or
\[
d(y,x_0)\geq\frac{R}{2} \hspace{1em}\text{ and }\hspace{1em} d(y,x_0)\geq{}d(x,x_0).\vspace{2em}
\]
This means \hspace{1em} $1_{\lbrace{}d(x,y)\geq{}R\rbrace} \leq 1_{\lbrace{}d(x,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(x,x_0)\geq{}d(y,x_0)\rbrace} + 1_{\lbrace{}d(y,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(y,x_0)\geq{}d(x,x_0)\rbrace}$ \\ 
and with the help of Jensen's inequality we obtain
\begin{align*}
&\hspace{-3em}\left(d(x,y)^p-R^p\right)_{+} = (d(x,y)^p-R^p)\,1_{\lbrace{}d(x,y)\geq{}R\rbrace} \\\leq\, & d(x,y)^p\,1_{\lbrace{}d(x,y)\geq{}R\rbrace} \\ \leq\,& d(x,y)^p\,1_{\lbrace{}d(x,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(x,x_0)\geq{}d(y,x_0)\rbrace} + d(x,y)^p\,1_{\lbrace{}d(y,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(y,x_0)\geq{}d(x,x_0)\rbrace} \vspace{0.4em}\\\leq\,& (d(x,x_0)+d(y,x_0))^p\,1_{\lbrace{}d(x,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(x,x_0)\geq{}d(y,x_0)\rbrace} \vspace{0.4em}\\+\,& (d(x,x_0)+d(y,x_0))^p\,1_{\lbrace{}d(y,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(y,x_0)\geq{}d(x,x_0)\rbrace} \\\leq\,& 2^pd(x,x_0)^p\,1_{\lbrace{}d(x,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(x,x_0)\geq{}d(y,x_0)\rbrace} + 2^pd(y,x_0)^p\,1_{\lbrace{}d(y,x_0)\geq{}\frac{R}{2} \,\text{ and }\, d(y,x_0)\geq{}d(x,x_0)\rbrace} \\ \leq\,& 2^pd(x,x_0)^p\,1_{\lbrace{}d(x,x_0)\geq\frac{R}{2}\rbrace} + 2^pd(y,x_0)^p\,1_{\lbrace{}d(y,x_0)\geq\frac{R}{2}\rbrace}.
\end{align*}
Using the sequence of optimal couplings $(\pi_k)_{k\in\mathbb{N}}$ of $(\mu_k,\mu)$, it follows
\begin{align*}
W_p(\mu_k,\mu)^p & = \int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_k(x,y) \\& = \int\limits_{\mathcal{X\times{}X}}\left(d(x,y)\wedge{}R\right)^p + \left(d(x,y)^p-R^p\right)_{+}d\pi_k(x,y) \\& \leq\int\limits_{\mathcal{X\times{}X}}\left(d(x,y)\wedge{}R\right)^pd\pi_k(x,y) + \int\limits_{d(x,x_0)\geq\frac{R}{2}}2^pd(x,x_0)^pd\pi_k(x,y) \\&\quad + \int\limits_{d(y,x_0)\geq\frac{R}{2}}2^pd(y,x_0)^pd\pi_k(x,y)\\& = \int\limits_{\mathcal{X\times{}X}}\left(d(x,y)\wedge{}R\right)^pd\pi_k(x,y) + \int\limits_{d(x,x_0)\geq\frac{R}{2}}2^pd(x,x_0)^pd\mu_k(x) \\&\quad + \int\limits_{d(y,x_0)\geq\frac{R}{2}}2^pd(y,x_0)^pd\mu(y).
\end{align*}
We know that $(\pi_k)_{k\in\mathbb{N}}$ converges weakly to a probability measure $\pi$ and since $\left(d(x,y)\wedge{}R\right)^p$ is bounded and continuous Portmanteau's theorem yields \vspace{0.3em}\\
\[
\lim\limits_{k\rightarrow\infty}\int\limits_{\mathcal{X\times{}X}}\left(d(x,y)\wedge{}R\right)^pd\pi_k(x,y) = \int\limits_{\mathcal{X\times{}X}}\left(d(x,y)\wedge{}R\right)^pd\pi(x,y) = 0.
\]
From this it follows
\begin{align*}
\limsup\limits_{k\rightarrow{}\infty}W_p(\mu_k,\mu)^p & \leq \limsup_{k\rightarrow\infty}\int\limits_{d(x,x_0)\geq\frac{R}{2}}2^pd(x,x_0)^pd\mu_k(x) \\& + \int\limits_{d(y,x_0)\geq\frac{R}{2}}2^pd(x_0,y)^pd\mu(y)
\end{align*}
and since $(\mu_k)_{k\in\mathbb{N}}$ converges weakly to $\mu$ in $P_p(\mathcal{X})$ it holds 
\begin{align*}
\lim\limits_{R\rightarrow\infty}\limsup\limits_{k\rightarrow\infty}\int\limits_{d(x_0,x)\geq\frac{R}{2}}d(x_0,x)^pd\mu_k(x) = 0
\end{align*}
and of course
\[
\lim\limits_{R\rightarrow\infty}\int\limits_{d(x_0,y)\geq\frac{R}{2}}d(x_0,y)^pd\mu_k(y) = 0,
\]
which yields $\limsup\limits_{k\rightarrow\infty}W_p(\mu_k,\mu)=0$. \vspace{1em}\\We have now proven that weak convergence in $P_p(\mathcal{X})$ implies convergence in the topology induced by the Wasserstein metric $W_p$, which gives us the desired equivalence.
\end{proof}
\noindent{}At this point we know that $W_p$ metrizes weak convergence on $P_p(\mathcal{X})$. This means for two weakly convergent sequences $(\mu_k)_{k\in\mathbb{N}}$ and $(\nu_k)_{k\in\mathbb{N}}$ in $P_p(\mathcal{X})$ with respective weak limits $\mu,\nu\in{}P_p(\mathcal{X})$, it holds
\[
W_p(\mu_k,\nu_k)\rightarrow{}W_p(\mu,\nu).
\]
This can be proven, by simply using the same argument as in the proof above.\vspace{1em}\\
On the other hand, if the two sequences only converge weakly in $P_p(\mathcal{X})$ in terms of regular weak convergence, we can use Lemma 5.2.1 and obtain $W_p(\mu,\nu)\leq\liminf\limits_{k\rightarrow\infty}W_p(\mu_k,\nu_k)$ as we did above. 
\subsection{Properties of the Wasserstein space}
When examining the Wasserstein distance, in many cases the underlying space was Polish. Some properties of the underlying space, especially in regard to its topology are transfered onto the Wasserstein space via the metric.
\subsubsection{Theorem}
Let $(\mathcal{X},d)$ be a Polish space and $p\in{}[1,\infty)$. Then the Wasserstein space $(P_p(\mathcal{X}),W_p)$ is a Polish space as well.
\begin{proof}
We have already proven, that $P_p(\mathcal{X})$ is a metric space and therefore restrict our attention to the separability and completeness.\vspace{1em}\\
We show the separability first:\\
Knowing that $(\mathcal{X},d)$ is a Polish space, we know there exists a countable and dense subset $\mathcal{D}\subset\mathcal{X}$. \vspace{1em}\\
Now define $\mathcal{P}$ as the set of all measures, for which there exists $N\in\mathbb{N}$,
\begin{align*}
x_1,\ldots{},x_N\in\mathcal{D}\quad{}\text{and}\quad{}a_1,\ldots,a_N\in\mathbb{Q} \quad\quad\text{with}\quad\quad \sum\limits_{i=1}^{N}a_i=1, 
\end{align*}
that are given by \hspace{0.3em}$\sum\limits_{i=1}^{N}a_i\delta_{x_i}.$\vspace{1.5em}\\
It is obvious that $\mathcal{P}\subset{}P_p(\mathcal{X})$, since for arbitrary $x_0$ it holds
\begin{align*}
\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu(x) = \sum\limits_{i=1}^{N}a_id(x_0,x_i)^p<\infty
\end{align*}
and that $\mathcal{P}$ is countable.
We will show that $\mathcal{P}$ is dense in $P_p(\mathcal{X})$ and conclude the separability of $P_p(\mathcal{X}).$\vspace{1em}\\
Take $\mu\in{}P_p(\mathcal{X})$, then we know that for arbitrary $\varepsilon>0$ and $x_0\in\mathcal{D}$, there exists a compact set $K\subset\mathcal{X}$ such that 
\begin{align}
\int\limits_{\mathcal{X}\backslash{}K}d(x_0,x)^pd\mu(x)\leq\varepsilon^p. 
\label{Compact}
\end{align}
This follows directly from the fact, that for $\mu\in{}P_p(\mathcal{X})$ it holds 
\begin{align*}
C:=\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu <\infty.
\end{align*}
Consequently we know that the mapping 
\begin{align*}
\mathcal{B(X)}\longrightarrow{}[0,1],\,A\mapsto\frac{1}{C}\int\limits_{A}d(x_0,x)^pd\mu
\end{align*}
is a probability measure in $\mathcal{P(X)}$. Knowing that every probability measure on a Polish space is tight, yields $(\ref{Compact})$.
Since $K$ is compact and $\mathcal{X}$ is separable, $K$ can be covered by finitely many balls $B(x_k,\varepsilon)$, with $x_k\in\mathcal{D}.$\vspace{1em}\\
Now define $B'_k = B(x_k,\varepsilon)\backslash{}B'_{k-1}$, with $B_1=B(x_1,\varepsilon),$ then all $B_k'$s are disjoint and still cover $K$.\vspace{1em}\\
We now define a function $f:\mathcal{X}\rightarrow\mathcal{X}$, by
\begin{align*}
f(B'_k\cap{}K)=\{x_k\},\qquad f(\mathcal{X}\backslash{}K)=\{x_0\}.
\end{align*}
Then for $x\in{}K$ it obviously holds $d(x,f(x))\leq\varepsilon$ and therefore
\begin{align*}
\int\limits_{\mathcal{X}}d(x,f(x))^pd\mu(x) = \int\limits_{K}d(x,f(x))^pd\mu(x) + \int\limits_{\mathcal{X}\backslash{}K}d(x,x_0)^pd\mu(x) \leq \varepsilon^p\mu(K) + \varepsilon^p = 2\varepsilon^p.
\end{align*}
We can see that f is a measurable function, since we're given the Borel $\sigma$-algebra. Then the tuple of random variables $(\text{Id},f)$ is a coupling of $\mu$ and $\mu\circ{}f^{-1} = \sum\limits_{i=1}^{N}\mu(B'_i)\delta_{x_i}$, since 
\begin{align*}
&\mu\circ{}(\text{Id},f)^{-1}(\mathcal{X}\times{}A) = \mu(\{x\in\mathcal{X} | (x,f(x))\in\mathcal{X}\times{}A\}) = \mu(f^{-1}(A))\vspace{0.5em}\\
\text{ and }\quad\quad{}&\vspace{0.5em}\\
&\mu\circ{}(\text{Id},f)^{-1}(A\times\mathcal{X}) = \mu(\lbrace{}x\in\mathcal{X} | (x,f(x))\in{}A\times\mathcal{X}\rbrace{}) = \mu(A).
\end{align*}
The Wasserstein distance between these two is bounded by
\begin{align*}
W_p(\mu,\mu\circ{}f^{-1})^p\leq\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\mu\circ{}(\text{Id},f)^{-1}(x,y) = \int\limits_{\mathcal{X}}d(x,f(x))^pd\mu(x)\leq{}2\varepsilon^p.
\end{align*} 
What we have shown now, is that for any $\varepsilon>0$ and any $\mu\in{}P_p(\mathcal{X})$ one can find $a_1,\ldots,a_N\in\mathbb{R}$ and $x_1,\ldots,x_N\in\mathcal{D}$ with $\nu=\sum\limits_{i=1}^{N}a_i\delta_{x_i}$, whereby $a_i=\mu(B'_i)$, such that $W_p(\mu,\nu)\leq{}2\varepsilon^p$. \vspace{1.5em}\\
Since $\mathbb{Q}$ is dense in $\mathbb{R}$ the $a_i$ can be approximated with arbitrary precision by $b_1,\ldots,b_N\in\mathbb{Q}.\,\,$ 
Take $N+1$ sequences $(b^{(k)}_i)_{k\in\mathbb{N}}\subset\mathbb{Q}$ $(0\leq{}i\leq{}N)$ such that $b^{(k)}_i\nearrow{}a_i$. Then define 
\begin{align*}
\nu_k:=\sum\limits_{i=0}^{N}b^{(k)}_i\delta_{x_i}\in{}\mathcal{P},
\end{align*}
which yields for $A\subset\mathcal{X}$ closed
\begin{align*}
\limsup\limits_{k\rightarrow\infty}\nu_k(A) = \limsup\limits_{k\rightarrow\infty}\sum\limits_{i=0}^{N}b^{(k)}_i\delta_{x_i}(A)\leq\sum\limits_{i=0}^{N}a_i\delta_{x_i}(A)=\nu(A).
\end{align*}
This means $(\nu_k)_{k\in\mathbb{N}}$ converges weakly to $\nu$ and because for arbitrary $\tilde{x}\in\mathcal{X}$ it holds
\begin{align*}
\limsup\limits_{k\rightarrow\infty}\int\limits_{\mathcal{X}}d(\tilde{x},x)^pd\nu_k(x) = \limsup\limits_{k\rightarrow\infty}\sum\limits_{i=0}^{N}b^{(k)}_i{}d(\tilde{x},x_i)^p\leq\sum\limits_{i=0}^{N}a_i{}d(\tilde{x},x_i)^p = \int\limits_{\mathcal{X}}d(\tilde{x},x)^pd\nu.
\end{align*}
Hence we know that $(\nu_k)_{k\in\mathbb{N}}$ converges weakly to $\nu$ in $P_p(\mathcal{X})$ and therefore $W_p(\nu_k,\nu)\rightarrow{}0$. What this has shown now is that any $\mu\in{}P_p(\mathcal{X})$ can be approximated by probability measures in $\mathcal{P}$ of the form given above, thereby proving the separability of $P_p(\mathcal{X})$.\vspace{1em}\\
We now move on to the completeness of $P_p(\mathcal{X})$:\\
Let $(\mu_k)_{k\in\mathbb{N}}\subset{}P_p(\mathcal{X})$ be a $W_p$-Cauchy sequence. Then by Lemma 5.1.2 we know that $(\mu_k)_{k\in\mathbb{N}}$ is tight. This means by Prokhorov's theorem there exists a weakly convergent subsequence $(\mu_{k'})_{k'\in\mathbb{N}}$ of $(\mu_k)_{k\in\mathbb{N}}$ with some weak limit $\mu\in{}\mathcal{P(X)}$. Given the fact that $(\mu_{k'})_{k'\in\mathbb{N}}\subset{}(\mu_{k})_{k\in\mathbb{N}}$ is also a Cauchy sequence in terms of the Wasserstein metric, we know that for every $\varepsilon>0$ there exists $N\in\mathbb{N}$ such that for every $k\in\mathbb{N}_{\geq{}N}$ it holds $W_p(\mu_N,\mu_k)\leq\varepsilon$. This means
\begin{align*}
\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu(x) &= W_p(\delta_{x_0},\mu)^p\\&\leq\left(W_p(\delta_{x_0},\mu_N) + W_p(\mu_N,\mu)\right)^p\\&\leq{}2^p\left(W_p(\delta_{x_0},\mu_N)^p + W_p(\mu_N,\mu)^p\right) \\&= 2^p\left(\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu_N(x)\right)^p + 2^p\varepsilon^p.
\end{align*}
It is $\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu_N(x)<\infty$ and therefore $\int\limits_{\mathcal{X}}d(x_0,x)^pd\mu(x)<\infty$, which means $\mu\in{}P_p(\mathcal{X}).$\vspace{1em}\\
The sequence $(\mu_{k'})_{k'\in\mathbb{N}}$ is weakly convergent with weak limit $\mu$, therefore we know that for every $l'\in\mathbb{N}$ it holds $W_p(\mu,\mu_{l'})\leq\liminf\limits_{k'\rightarrow\infty}W_p(\mu_{k'},\mu_{l'})\leq\limsup\limits_{k'\rightarrow\infty}W_p(\mu_{k'},\mu_{l'})$. Adding another $\limsup$ yields
\[
\limsup\limits_{l'\rightarrow\infty}W_p(\mu,\mu_{l'})\leq\limsup\limits_{l'\rightarrow\infty}\limsup\limits_{k'\rightarrow\infty}W_p(\mu_{k'},\mu_{l'}).
\]
Since $(\mu_{k'})_{k'\in\mathbb{N}}$ is a Cauchy sequence in terms of the Wasserstein metric it is 
\[
\limsup\limits_{l'\rightarrow\infty}\limsup\limits_{k'\rightarrow\infty}W_p(\mu_{k'},\mu_{l'})=0.
\]
And therefore 
\[
\limsup\limits_{l'\rightarrow\infty}W_p(\mu,\mu_{l'})=0
\]
which means $(\mu_{k'})_{k'\in\mathbb{N}}$ is actually weakly convergent in terms of weak convergence in $P_p(\mathcal{X}).$
Thereby we have a Cauchy sequence with a convergent subsequence and hence $(\mu_k)_{k\in\mathbb{N}}$ converges weakly in $P_p(\mathcal{X}).$ Subsequently every Cauchy sequence in $P_p(\mathcal{X})$ converges, i.e $P_p(\mathcal{X})$ is complete.
\end{proof}
\subsubsection{Remark}
If $(\mathcal{X},d)$ is compact, then the Wasserstein space $(P_p(\mathcal{X}),W_p(\mathcal{X}))$ is also compact.
\begin{proof}
We know that if $(\mathcal{X},d)$ is compact, it holds $\sup\limits_{x\in\mathcal{X}}d(x_0,x)<\infty$ for all $x_0\in\mathcal{X}$. From this we conclude that 
\[
\forall x_0\in\mathcal{X}\quad\int\limits_{\mathcal{X}}d(x,x_0)d\mu(x)<\infty.
\]
This implies $P_p(\mathcal{X})=\mathcal{P(X)}$, whereby we recall that $\mathcal{P(X)}$ denotes the set of all probability measures on $\mathcal{X}$.\vspace{1em}\\
For any $\mu\in\mathcal{P(X)}$ it obviously holds $\mu(\mathcal{X}\backslash\mathcal{X})=\mu(\varnothing)=0$. Since $\mathcal{X}$ is compact we know that for every $\varepsilon>0$ there exists a compact $K\subset\mathcal{X}$ sucht that $\sup\limits_{\mu\in\mathcal{P(X)}}\mu(K^c)<\varepsilon$ and therefore $\mathcal{P(X)}$ is tight. By Prokhorov's theorem $\mathcal{P(X)}$ is relatively sequentially compact and since $(\mathcal{P(X)},W_p(\mathcal{X}))$ is a closed metric space it is compact. 
\end{proof}
\noindent{}For instance, the Wasserstein space over the euclidean space $\mathbb{R}^n$ is a Polish space, but not compact, whereas the Wasserstein space over a closed intervall $[a,b]$ is Polish and compact. It is then of course simply given by $\mathcal{P}([a,b])$.
\subsubsection{Remark}
$(\mathcal{X},d)$ locally compact does not necessarily imply $(P_p(\mathcal{X}),W_p(\mathcal{X}))$ locally compact.
\begin{proof}
A good example of this is given by $\mathbb{N}_0$ endowed with the discrete metric. This is a locally compact topological space, since the sets, given by $\lbrace{}n\rbrace$ are open and compact. Because the discrete metric is bounded by 1, we know that $P_p(\mathbb{N}_0)=\mathcal{P}(\mathbb{N}_0).$ Then $(\mathcal{P}(\mathbb{N}_0),W_1)$ is not locally compact. To show this we note, that on a Hausdorff space local compactness is equivalent to every point having a precompact neighbourhood. By Prokhorov this is equivalent to every point having a tight neighbourhood. For every $\varepsilon>0$ the open Ball $B(\delta_0,\varepsilon)$ contains the sequence $(\mu_n)_{n\in\mathbb{N}}$, given by
\begin{align*}
\mu_n:=(1-\tilde{\varepsilon})\delta_0 + \tilde{\varepsilon}\delta_{x_n}
\end{align*} 
for a $\tilde{\varepsilon}<\varepsilon$, because
\begin{align*}
W_1(\delta_0,\mu_n) = \int\limits_{\mathcal{X}}d(0,x)d\mu_n = \tilde{\varepsilon}\underbrace{d(0,n)}_{=1}+(1-\tilde{\varepsilon})\underbrace{d(0,0)}_{=0}<\varepsilon.
\end{align*}
It is obvious that the sequence $(\mu_n)_{n\in\mathbb{N}}$ isn't tight. Hence $\delta_0$ does not have a tight neighbourhood and $(\mathcal{P}(\mathbb{N}_0),W_1)$ isn't locally compact.
\end{proof}
\noindent{}Now let's take a look at the Wasserstein space over the euclidean space $\mathbb{R}$.
\section{The Wasserstein metric on $\mathbb{R}$}
We have now understood most properties of the Wasserstein distance, especially in regard to the topology it induces on the Wasserstein space over Polish spaces. Probably the most well-known Polish space is the euclidean space $\mathbb{R}^d$, endowed simply with the standard metric. We know that the Wasserstein space over $\mathbb{R}^d$ is a Polish space. 
\subsection{The distance between distribution functions}
\subsubsection{Theorem}
Let $(\mathbb{R},d)$ be the one-dimensional euclidean space, endowed with the standard metric. Given $\mu,\nu\in{}P_p(\mathbb{R})$ we define the distribution functions $F$ and $G$ of $\mu$ and $\nu$, by $F(x)=\mu((-\infty,x])$ and $G(x)=\nu((-\infty,x])$. \vspace{1em}\\Then the Wasserstein distance between $\mu$ and $\nu$ is given by
\[
W_1(\mu,\nu)\hspace{0.3em} \substack{(i)\\=} \int\limits_{-\infty}^{\infty}|F(x)-G(x)|\,dx\hspace{0.3em}\substack{(ii)\\=}\hspace{0.3em} \int\limits_{0}^{1}|F^{-1}(t)-G^{-1}(t)|\,dt. 
\]
\begin{proof}
Let's start with $(i).$\\ Recall the Kantorovich-Rubinstein formula for $W_1$ 
\begin{align*}
W_1(\mu,\nu) &= \sup\limits_{\|\phi\|_{\text{Lip}}\leq{}1}\left(\int\limits_{\mathbb{R}}\phi(x)d\mu(x) - \int\limits_{\mathbb{R}}\phi(y)d\nu(y)\right)\,=\,\sup\limits_{\|\phi\|_{\text{Lip}}\leq{}1}\int\limits_{\mathbb{R}}\phi(x)d(\mu-\nu)(x).
\end{align*}
Note that a 1-Lipschitz function $\phi$ is differentiable almost everywhere as well as integrable. We define $f_{\eta}$ as the density function of $\eta:=\mu-\nu$. Then the antiderivative of $f_{\eta}$ is given by $h(r)=\eta((-\infty,r])$. Partial integration and the use of the funtion $f_\eta$, then yields
\begin{align*}
\int\limits_{\mathbb{R}}\phi(x)d\eta(x) = \int\limits_{\mathbb{R}}\phi(r)f_\eta(r)dr = \left[\phi(r)h(r))\right]_{r = -\infty}^{\infty} - \int\limits_{-\infty}^{\infty}\phi'(r)h(r)dr.
\end{align*}
For $h(x)=F(x)-G(x)$ define $g:\mathbb{R}\rightarrow{}[-1,1]$ by
\[
g(x):=\begin{cases}
1,\, & F(x)>G(x)\\
0,\,& F(x)=G(x) \\
-1,\, & F(x)<G(x) 
\end{cases}.
\]
Now define 
\[
f(x):=\int\limits_{-\infty}^{x}g(r)dr.
\] 
Then $f$ is Lipschitz continuous with Lipschitz constant 1, since for $x\geq{}y$ it holds
\[
\left|f(x)-f(y)\right|=\left|\int\limits_{-\infty}^{x}g(r)dr - \int\limits_{-\infty}^{y}g(r)dr\right|=\left|\int\limits_{y}^{x}g(r)dr\right|\leq\int\limits_{y}^{x}|g(r)|dr\leq\int\limits_{x}^{y}1dr=|x-y|.
\]
Analogously for $x\leq{}y$. \vspace{1em}\\
Using partial integration, we obtain
\begin{align*}
W_1(\mu,\nu)&\geq{}\int\limits_{-\infty}^{\infty}f(x)d\mu(x) - \int\limits_{-\infty}^{\infty}f(x)\nu(x) \\& = \int\limits_{-\infty}^{\infty}f(x)d\eta(x) \\&= \int\limits_{-\infty}^{\infty}f(r)f_{\eta}(r)dr \\&= \int\limits_{-\infty}^{\infty}f(r)h'(r)dr \\& = \left[f(r)h(r)\right]_{r = -\infty}^{\infty} - \int\limits_{-\infty}^{\infty}f'(r)h(r)dr.
\end{align*}
Recall that $h(\infty)=\mu(\mathbb{R})-\nu(\mathbb{R})=0$ and $h(-\infty)=\mu(\varnothing)-\nu(\varnothing)=0$, which gives us
\begin{align*}
\left[f(r)h(r)\right]_{r=-\infty}^{\infty} = h(\infty)\int\limits_{-\infty}^{\infty}g(r)dr - h(-\infty)\int\limits_{-\infty}^{-\infty}g(r)dr = h(\infty)\int\limits_{-\infty}^{\infty}g(r)dr \leq \int\limits_{-\infty}^{\infty}h(\infty)dr = 0.
\end{align*}
We therefore obtain 
\[
W_1(\mu,\nu)\geq\int\limits_{\mathbb{R}}f'(x)h(x)dx = \int\limits_{\mathbb{R}}g(x)h(x)dx = \int\limits_{\mathbb{R}}|h(x)|dx = \int\limits_{\mathbb{R}}\left|F(x)-G(x)\right|dx.
\]
For the inversed inequality, note that for a 1-Lipschitz continuous function $\phi$ it holds $|\phi'|\leq{}1\,\eta$-a.e. and we obtain
\begin{align*}
W_1(\mu,\nu)&=\sup\limits_{\|\phi\|_\text{Lip}\leq{}1}\int\limits_{\mathbb{R}}\phi(x)d\eta(x) = \sup\limits_{\|\phi\|_\text{Lip}\leq{}1}\left|\int\limits_{\mathbb{R}}\phi(x)d\eta(x)\right| \leq \sup\limits_{\|\phi\|_\text{Lip}\leq{}1}\left|\int\limits_{\mathbb{R}}\phi'(x)h(x)dx\right| \\&\leq \sup\limits_{\|\phi\|_\text{Lip}\leq{}1}\int\limits_{\mathbb{R}}|\phi'(x)||h(x)|dx \leq \sup\limits_{\|\phi\|_\text{Lip}\leq{}1}\int\limits_{\mathbb{R}}|h(x)|dx = \int\limits_{\mathbb{R}}\left|F(x)-G(x)\right|dx
\end{align*}
and we conclude 
\begin{align*}
W_1(\mu,\nu)=\int\limits_{-\infty}^{\infty}|F(x)-G(x)|dx.
\end{align*}
\vspace{1em}\\
For $(ii)$ we define the inverted distribution function as 
\begin{align*}
F^{-1}(t) := \inf\lbrace{}x\in\mathbb{R}\,:\,F(x)\geq{}t\rbrace.
\end{align*}
We also define $a(x):=\min\lbrace{}F(x),G(x)\rbrace$ and $b(x):=\max\lbrace{}F(x),G(x)\rbrace$ and gain
\begin{align*}
\int\limits_{\mathbb{R}}|F(x)-G(x)|dx &= \int\limits_{\mathbb{R}}b(x)-a(x)dx = \int\limits_{\mathbb{R}}\int\limits_{0}^{b(x)}1\,dt - \int\limits_{0}^{a(x)}1\,dtdx \\& = \int\limits_{\mathbb{R}}\int\limits_{0}^{1}1_{[t\leq{}b(x)]}(t) - 1_{[t\leq{}a(x)]}(t) dtdx \\& \substack{\text{Fubini}\\=} \int\limits_{0}^{1}\int\limits_{\mathbb{R}} 1_{[t\leq{}b(x)]}(t) - 1_{[t\leq{}a(x)]}(t) dxdt.  
\end{align*}
We know that it holds
\begin{align*}
\max\lbrace{}F(t),G(t)\rbrace{}^{-1}&= \inf\lbrace{}x\in\mathbb{R}\,:\,\max\lbrace{}F(x),G(x)\rbrace\geq{}t\rbrace \\&= \max\lbrace{}\inf\lbrace{}x\in\mathbb{R}\,:\,F(x)\geq{}t\rbrace,\inf\lbrace{}x\in\mathbb{R}\,:\,G(x)\geq{}t\rbrace\rbrace \\&= \max\lbrace{}F^{-1}(t),G^{-1}\rbrace
\end{align*}
and of course it is 
\[
\lbrace{}t\leq{}\max\lbrace{}F(x),G(x)\rbrace\rbrace = \lbrace\max\lbrace{}F(t),G(t)\rbrace^{-1}\geq{}x\rbrace.
\]
Using the monotonicity of $F$ and $G$ it follows directly from this
\begin{align*}
\int\limits_{\mathbb{R}}\left|F(x)-G(x)\right|dx &= \int\limits_{0}^{1}\int\limits_{\mathbb{R}}1_{[b^{-1}(t)\geq{}x]}(t)-1_{[a^{-1}(t)\geq{}x]}(t)dxdt \\&= \int\limits_{0}^{1}\int\limits_{a^{-1}(t)}^{b^{-1}(t)}1\,dxdt \\&= \int\limits_{\mathbb{R}}b^{-1}(t)-a^{-1}(t)dt \\&= \int\limits_{0}^{1}|F^{-1}(t)-G^{-1}(t)|dt.
\end{align*}
This yields the two desired equations. 
\end{proof}
\noindent{}As mentioned before this metric is also referred to as the Kantorovich-Rubinstein metric.\\
As a corollary one should add that for $p\geq{}1$ the same equation also holds
\[
W_p(\mu,\nu)^p = \int\limits_{-\infty}^{\infty}|F(x)-G(x)|^pdx = \int\limits_{0}^{1}|F^{-1}(x)-G^{-1}(x)|^pdx.
\]
\subsection{Particular examples of the Wasserstein distance}
This perfectly expemplifies how the Wasserstein distance measures the distance between probability measures. Note, that it is not only limited to measures, but also quantifies the distance between probability distributions and therefore random variables too. Before we move on to the next probability distance, we look at a few examples of the Wasserstein distance between probability distributions.
\subsubsection{Example}
One of the most common distributions in probability theory is the normal distribution. Given a probability measure $\mu\in\mathcal{P}(\mathbb{R})$ with distribution function $F(x)=\mu((-\infty,x])$ it might be interesting to gauge the discrepancy between $F$ and the normal distribution. Take the density function of the standard normal distribution $\phi(x):=\frac{1}{\sqrt{2\pi}}\exp(-{\frac{x^2}{2}})$ and the distribution function given by $\Phi(x):=\int\limits_{-\infty}^{x}\phi(t)dt$, then the space of normal probability distributions, with expected value $\kappa\in\mathbb{R}$ and standard deviation $\sigma>0$ could be denoted
\begin{align*}
\mathcal{H}:=\left\lbrace{}H;\,H(x)=\Phi(\frac{x-\kappa}{\sigma}), \kappa\in\mathbb{R},\sigma>0\right\rbrace.
\end{align*}
The distance between the distribution function $F$ of $\mu$ and $\mathcal{H}$ with respect to the Wasserstein distance is then given by 
\begin{align*}
W_p(F,\mathcal{H})^p:=&\inf\left\lbrace{}W_p(F,H)^p;\,H\in\mathcal{H}\right\rbrace \\[5pt]=& \inf\limits_{H\in\mathcal{H}}\int\limits_{-\infty}^{\infty}|F(x)-H(x)|^pdx\\[5pt]=& \inf\limits_{H\in\mathcal{H}}\int\limits_{0}^{1}|F^{-1}(t)-H^{-1}(t)|^pdt \\[5pt]=& \inf\limits_{\substack{\kappa\in\mathbb{R}\\\sigma>0}}\int\limits_{0}^{1}|F^{-1}(t) - \sigma\Phi^{-1}(t) - \kappa|^pdt.
\end{align*}  
This means for two normal distributions $F$ and $G$ with standard deviations $\sigma_1\geq\sigma_2$ and expected values $\kappa_1\geq\kappa_2$, the Wasserstein distance between the two is given by
\begin{align*}
W_1(F,G)&=\int\limits_{0}^{1}|\sigma_1\Phi^{-1}(t)+\kappa_1-\sigma_2\Phi^{-1}(t)-\kappa_2|dt \\=&(\sigma_1-\sigma_2)\int\limits_{0}^{1}|\Phi^{-1}(t)+\frac{\kappa_1-\kappa_2}{\sigma_1-\sigma_2}|dt \\=& (\sigma_1-\sigma_2)\int\limits_{0}^{1}|\Phi^{-1}(t)|dt + \kappa_1-\kappa_2.
\end{align*}
\subsubsection{Example}
Another interesting particular example of the Wasserstein distance over $\mathbb{R}$, is given by a sequence of normal distributions $(F_n)_{n\in\mathbb{N}}$ with constant expected value $\kappa$ and a sequence of standard deviations $(\sigma_n)_{n\in\mathbb{N}}$ such that $\lim\limits_{n\rightarrow\infty}\sigma_n=0$. The limit of a sequence like that would be given by the distribution function
\begin{align*} 
H(x):=\begin{cases}0\,&\,x<\kappa\\1\,&\,x\geq\kappa\end{cases}.
\end{align*}
Without loss of generality we assume $\kappa=0$, then the Wasserstein distance yields
\begin{align*}
W_1(F_n,F)&=\int\limits_{-\infty}^{\infty}|\Phi(\frac{x}{\sigma_n})-1_{[x\geq{}0]}(x)|dx=\int\limits_{-\infty}^{0}\Phi(\frac{x}{\sigma_n})dx + \int\limits_{0}^{\infty}1-\Phi(\frac{x}{\sigma_n})dx \\&= 2\int\limits_{0}^{\infty}1-\Phi(\frac{x}{\sigma_n})dx = 2\sigma_n\int\limits_{0}^{\infty}1-\Phi(x)dx\,\longrightarrow\,0.
\end{align*} 
Hence a sequence of probability measures related to the normal distribution converges weakly to the dirac measure, concentrated in $0$, if the standard deviation converges to $0$.
The distance between $\mu$ and other probability distributions in terms of the Wasserstein distance is computed analogously. 
\subsubsection{Example}
Of course, if for a sequence $(\mu_n)_{n\in\mathbb{N}}$ the upper distance $W_p(F_n,\mathcal{H})$ goes to zero, we see that the given sequence converges weakly to a normal distribution. \\
For instance, take a sequence of iid random variables $(X_n)_{n\in\mathbb{N}}$ on $\mathbb{R}$ with expected value $\kappa$ and standard deviation $\sigma$ with respect to some probability measure $\mathbb{P}$, whereby the sequence of push-forward measures $(\mu_n)_{n\in\mathbb{N}}$, given by $\mu=\mathbb{P}\circ{}X_n^{-1}$ is in the Wasserstein space. Then the central limit theorem states that for 
\begin{align*}
\nu_n:=\mathbb{P}\circ\left(\frac{\sum\limits_{i=1}^{n}X_i-n\kappa}{\sigma\sqrt{n}}\right)^{-1}
\end{align*}
it holds  
\begin{align*}
W_p(\nu_n,\mathcal{H})\longrightarrow{}0.
\end{align*}
One can approach the law of large numbers in a similar fashion.\vspace{1em}\\
This concludes the segment about the Wasserstein distance. While being one of the most commonly used probability metrics, it is by far not the only one. There are a variety of other probability distances with similar topological properties. It is easy to see why the Wasserstein distance is so popular, though. The metrization of weak convergence and the transference of properties of the underlying metric space onto the Wasserstein space are two very useful aspects of this probability metric. The only downside is the restriction of $\mathcal{P(X)}$ to $P_p(\mathcal{X})$. It would be very useful to find a probability metric that metrizes weak convergence on the space of all probability measures and not just a subspace of it. This brings us to the next important probability metric, the Total Variation distance.
\section{Total Variation}
Let $(\mathcal{X,F})$ be a measurable space.\vspace{1em}\\
We now introduce another distance on the space of all probability measures $\mathcal{P(X)}$ on $\mathcal{F}$. Given two probability measures $\mu,\nu\in\mathcal{P(X)}$, then we define the Total Variation distance as
\begin{align*}
\|\mu-\nu\|_{TV}:=\inf\limits_{\pi\in\Pi(\mu,\nu)}\left(\int\limits_{\mathcal{X\times{}X}}1_{[x\neq{}y]}(x,y)d\pi(x,y)\right), 
\end{align*}
whereby we take the infimum over all couplings $\pi$ of $(\mu,\nu)$. Note the similarity to the Wasserstein distance and the Kantorovich duality. In this case we're given the cost function $c=1_{[x\neq{}y]}$. This is of course equal to
\begin{align*}
\|\mu-\nu\|_{TV} = \inf\bigg\lbrace\mathbb{P}\left(X\neq{}Y\right)\Big|\,\,(X,Y)\text{ coupling of }(\mu,\nu)\bigg\rbrace.
\end{align*}
Here $\mathbb{P}$ is a probability measure on some space $(\Omega,\mathbb{P})$, such that the coupling $(X,Y)$ satisfies
\begin{align*}
\mathbb{P}\circ{}X^{-1} = \mu\quad\quad\&\quad\quad\mathbb{P}\circ{}Y^{-1} = \nu.
\end{align*}
One should add, that the given cost function satisfies the requirements of Theorem 2.4.1 and thus the infimum is attained and therefore a minimum.
We see that our cost function satisfies the conditions of theorem 3.4.1 and therefore we know it can be rewritten as
\begin{align*}
||\mu-\nu||_{TV} &= \sup\limits_{\substack{(\psi,\phi)\in{}L^1(\mu)\times{}L^1(\nu)\\\psi-\phi\leq{}1_{[x\neq{}y]}}}\int\limits_{\mathcal{X}}\psi(x)d\mu(x)-\int\limits_{\mathcal{X}}\phi(y)d\nu(y)\\[15pt]& = \sup\limits_{\psi\in{}L^1(\mu)}\int\limits_{\mathcal{X}}\psi(x)-\psi^c(x)d(\mu-\nu)(x).
\end{align*}
For $\psi$ c-convex with c-transform $\psi^c$. \\
Let's take a closer look at what c-convexity pertaining to the given cost function $1_{[x\neq{}y]}$ means.\\
For $\psi$ c-convex with c-transform $\psi^c$ it holds
\begin{align*}
\psi^c(x)-\psi(x)\leq{}1_{[x\neq{}y]}(x,x)=0 \qquad \text{f.a. }x\in\mathcal{X}
\end{align*}
and therefore $\psi^c=\psi$. This means for arbitrary $x,y\in\mathcal{X}$ we obtain
\[
|\psi(x)-\psi(y)|\leq{}1.
\]
Hence the set of c-convex functions is given by all 
\begin{align*}
\psi:\mathcal{X}\rightarrow\mathbb{R}
\end{align*}
satisfying
\begin{align*}
\psi(x)-\psi(0)\leq{}1\quad\text{for all}\,\,\, x\in\mathcal{X}.
\end{align*}
Additionally we know that 
\[
\mu(\mathcal{X})-\nu(\mathcal{X})=0
\]
and conclude that for an arbitrary function $\psi$ and $a\in\mathbb{R}$ it holds
\[
\int\limits_{\mathcal{X}}\psi(x)+ad\mu(x)-\int\limits_{\mathcal{X}}\psi(y)+ad\nu(y) = \int\limits_{\mathcal{X}}\psi(x)d\mu(x)-\int\limits_{\mathcal{X}}\psi(y)d\nu(y).
\]
With this in mind we can restrict our attention to the supremum over all c-convex functions with $\psi(0)=0$. Thus we obtain
\[
||\mu-\nu||_{TV}=\sup\limits_{\|\psi\|_{\infty}\leq{}1}\int\limits_{\mathcal{X}}\psi(x)d(\mu-\nu)(x).
\]
Recall the additional assertion of the Kantorovich duality (theorem 3.4.1), stating that if the given cost function $c$ was bounded two functions $c_1\in{}L^1(\mu)$ and $c_2\in{}L^1(\nu)$, then the given supremum was attained. Since $1_{[x\neq{}y]}$ is bounded from above by $1$, we know that the upper supremum is a maximum.
\[
||\mu-\nu||_{TV}=\max\limits_{\|\psi\|_{\infty}\leq{}1}\int\limits_{\mathcal{X}}\psi(x)d(\mu-\nu)(x)
\]
Before we acquaint ourselves with another not quite so trivial identity of the Total Variation distance, we introduce the Hahn-Jordan decomposition of signed measures.
\subsection{Hahn and Jordan decomposition of signed measures}
Recall that a measure was a nonnegative, real-valued and $\sigma$-additive function defined on a measurable space $(\mathcal{X},\mathcal{F})$. We broaden this concept to include so called signed measures.\vspace{1em}\\
A signed measure is a $\sigma$-additive function $\mu:\mathcal{F}\rightarrow\overline{\mathbb{R}}$ that satisfies 
\begin{align*}
\mu(\varnothing)=0.
\end{align*}
We additionally impose that $\mu$ assumes at most one of the values $-\infty$ and $\infty$. This is to prevent the case where $\infty-\infty$ occurs. And given a sequence of disjoint measurable sets $(A_n)_{n\in\mathbb{N}}$ for which it holds
\begin{align*}
\mu\left(\dot{\bigcup\limits_{n\in\mathbb{N}}}A_n\right) = \sum\limits_{n\in\mathbb{N}}\mu(A_n)<\infty,
\end{align*}
we require that the series $\sum\limits_{n\in\mathbb{N}}\mu(A_n)$ converges absolutely. In that case $(\mathcal{X,F},\mu)$ is called a signed measure space.\vspace{2em}\\
We say that a measurable set $A\in\mathcal{F}$ is positive with respect to a signed measure $\mu$ if for all measurable subsets $B$ of $A$ it holds
\begin{align*}
\mu(B)\geq{}0.
\end{align*}
Similarly, $A$ is called negative if for all measurable subsets $B$ of $A$ it holds
\begin{align*}
\mu(B)\leq{}0.
\end{align*}
A set is called nullset if it is both positive and negative.
\subsubsection{Lemma}
Let $A$ be a measurable set, with $\mu(A)>0$ and $\mu(A)<\infty$, then there exists a positive set $B\subset{}A$, with $\mu(B)>0$.
\begin{proof}
If $A$ is itself positive then we are done. Suppose $A$ is not positive, then it has a measurable subset $E$ with $\mu(E)<0.$\vspace{1em}\\
Now take $n_1\in\mathbb{N}$ whereby $n_1$ is the smallest integer greater than zero, such that there exists a measurable subset of $A$ with $\mu(E_1)<-\frac{1}{n_1}$, whereby we let $E_1$ be the greatest of subsets, satisfying the given inequality.\vspace{1em}\\ For such an $E_1$ it holds
\begin{align*}
\mu(A\backslash{}E_1) = \mu(A)-\mu(E_1)>\mu(A)>0.
\end{align*}   
If $A\backslash{}E_1$ is positive, then we are done. If $A\backslash{}E_1$ is not, then we continue by taking the smallest $n_2\geq{}n_1$ such that there exists $E_2\subset{}A\backslash{}E_1$ with
\begin{align*}
\mu(E_2)<-\frac{1}{n_2},
\end{align*}  
whereby we let $E_2$ be the largest of subsets of $A\backslash{}E_1$ satisfying the upper condition. \\
If $A\backslash{}\left(E_1\cup{}E_2\right)$ is not positive either we continue inductively. \vspace{1em}\\If we reach a $K\in\mathbb{N}$ such that $A\backslash\bigcup\limits_{k=1}^{K-1}E_{n_k}$ is positive, then we are of course done. \vspace{1em}\\However if we never stop, we take
\begin{align*}
B:=A\backslash{}\bigcup\limits_{k\in\mathbb{N}}E_{n_k}
\end{align*}
and show that $B$ is positive. Notice that the sequence $(E_{n_k})_{k\in\mathbb{N}}$ is pairwise disjoint and of course $B$ and $\bigcup\limits_{k\in\mathbb{N}}E_{n_k}$ are disjoint. We obtain
\begin{align*}
\mu(A)=\mu(B)+\mu\left(\dot{\bigcup\limits_{k\in\mathbb{N}}}E_{n_k}\right) = \mu(B)+\sum\limits_{k\in\mathbb{N}}\mu(E_{n_k})<\infty.
\end{align*}
Due to the finiteness of $\mu(A)$ we conclude that the upper series $\sum\limits_{k\in\mathbb{N}}\mu(E_{n_k})$ must converge absolutely, which of course implies 
\begin{align*}
\lim\limits_{k\rightarrow\infty}\mu(E_{n_k}) = 0\qquad \text{hence}\qquad\lim\limits_{k\rightarrow\infty}n_k = 0.
\end{align*}
Take an arbitrary $\varepsilon>0$ then we know that there exists $K\in\mathbb{N}$ such that $\frac{1}{n_K}<\varepsilon$, hence $-\varepsilon<-\frac{1}{n_K}$. \vspace{1.5em}\\If there was a $E\subset{}B$ such that $\mu(B)<-\varepsilon$, then $E_{n_K}$ could not have been the greatest subset of $A\backslash\bigcup\limits_{k=1}^{K-1}E_{n_k}$ with measure less than $-\frac{1}{n_K}$, because $E_{n_K}\cup{}E$ would have been bigger with
\begin{align*}
\mu(E\cup{}E_{n_K}) < -\frac{1}{n_K}.
\end{align*} 
This, of course is a contradiction to the definition of $B$. Since $\varepsilon>0$ was arbitrary we now know that every subset of $B$ has value greater or equal to zero and thus $B$ is positive. \vspace{1em}\\
It also holds 
\begin{align*}
\mu(B) = \mu(A)-\sum\limits_{n\in\mathbb{N}}\underbrace{\mu(E_{n_k})}_{<0} > \mu(A) > 0.
\end{align*}
\end{proof}
\subsubsection{Theorem (Hahn Decomposition)}
Let $(\mathcal{X,F},\mu)$ be a signed measure space, then there is a positive set $P\in\mathcal{F}$ and a negative set $N\in\mathcal{F}$, such that
\begin{align*}
P\cup{}N = \mathcal{X}\qquad \text{ and }\qquad P\cap{}N=\varnothing.
\end{align*}
This is known as a Hahn Decomposition of signed measures.
\begin{proof}
Without loss of generality we may assume that $\mu<\infty$. Define $p:=\sup\limits_{P\,:\,P\text{ is positive}} \mu(P)$. \vspace{1em}\\Given that $\varnothing$ is positive , we know $p\geq{}0$. By definition of the supremum, there exists a sequence $(P_n)_{n\in\mathbb{N}}$ of positive sets $P_n\subset\mathcal{X}$, such that 
\begin{align*}
p = \lim\limits_{n\rightarrow\infty}\mu(P_n).
\end{align*}
We define $P:=\bigcup\limits_{n\in\mathbb{N}}P_n$ then $P$ is obviously positive and therefore it holds $\mu(P)\leq{}p$. \vspace{1em}\\Since it holds for any $n\in\mathbb{N}$
\begin{align*}
\mu(P) = \mu(P_n) + \mu(P\backslash{}P_n) \geq \mu(P_n),
\end{align*}
we know that $p=\mu(P)<\infty$. \vspace{1em}\\Now let $N=\mathcal{X}\backslash{}P$. We claim that $N$ is negative. Assume there exists a $B\subset{}N$ with $\mu(B)>0$, then $B$ and $P$ are disjoint and by lemma 7.1.1 there exists a positive $\tilde{B}\subset{}B$ with $\mu(\tilde{B})>0$. Then of course $\tilde{B}\cup{}P$ is positive as the union of two positive sets. We deduce 
\begin{align*}
p\geq{}\mu(P\,\dot{\cup}\,\tilde{B})=\mu(P)+\mu(\tilde{B})\geq\mu(P)=p.
\end{align*}
This shows $\mu(\tilde{B})=0$, which was a contradiction to our assumption. Therefore $N$ is negative and we have our decomposition.\end{proof}
\noindent{}We now know that every signed measure has a Hahn decomposition. While the existence is now guaranteed, the uniqueness is not. Note that a positive/negative set can be united with a nullset and remain positive/negative. Hence the Hahn decomposition is only unique up to nullsets.\vspace{2em}\\
This brings us to the Jordan decomposition of signed measures.
\subsubsection{Jordan Decomposition}
Given a signed measure $\mu$ with a Hahn decomposition $P$ and $N$, we define 
\begin{align*}
\mu^{+}:=\mu(\,\cdot\,\cap{}P)\qquad \text{and}\qquad \mu^{-}:=-\mu(\,\cdot\,\cap{}N). 
\end{align*}
It is $\mu = \mu^{+}-\mu^{-}$. This is known as a Jordan decomposition of $\mu$ and we define
\begin{align*}
|\mu| = \mu^{+}+\mu^{-}\geq{}0.
\end{align*}
Note that while the Hahn decomposition of a signed measure was only unique up to nullsets, the Jordan decomposition is unique and independent of the underlying Hahn decomposition, since the only difference between $N$ and $P$ and another Hahn decomposition $A$ and $B$ are nullsets, which implies 
\begin{align*}
&\mu(\,\cdot\,\cap{}P) = \mu(\,\cdot\,\cap{}A)\\[3pt]\text{and }\,&\\&\mu(\,\cdot\,\cap{}N) = \mu(\,\cdot\,\cap{}B).
\end{align*}
Hence $\mu^{+}$ and $\mu^{-}$ are unique.\vspace{2em}\\
This brings us to another not quite so trivial identity of the Total Variation distance.
\subsubsection{Theorem}
The Total Variation distance is given by
\[
||\mu-\nu||_{TV} = \max\limits_{\psi:\mathcal{X}\rightarrow{}[-1,1]}\int\limits_{\mathcal{X}}\psi(x)d\mu(x) - \int\limits_{\mathcal{X}}\psi(x)d\nu(x) = 2\sup\limits_{A\subset\mathcal{X}}|\mu(A)-\nu(A)|. 
\]
\begin{proof}
Given two measures $\mu$ and $\nu$, the difference $\mu-\nu$ is a signed measure. Hence there is a Hahn decomposition $N$ and $P$ of $\mu-\nu$.
It also holds $0=\mu(\mathcal{X})-\nu(\mathcal{X}) = \mu(P)+\mu(N)-\nu(P)-\nu(N).$ And therefore $\mu(P)-\nu(P) = \nu(N)-\mu(N).$ Then the supremum on the right-hand side is achieved by either $P$ or $N$. Therefore we are granted a maximum on the right side.\vspace{1em}\\
The maximum in the middle is attained by $1_{P}-1_{N}$.\vspace{1em}\\
We obtain
\begin{align*}
||\mu-\nu||_{TV} &= \sup\limits_{\psi:\mathcal{X}\rightarrow{}[-1,1]}\int\limits_{\mathcal{X}}\psi(x)d(\mu-\nu)(x) \\&= \int\limits_{\mathcal{X}}1_{P}-1_{N}d(\mu-\nu)(x) \\&= \mu(P)-\nu(P) - \mu(N)+\nu(N) \\[3pt]&= 2|\mu(P)-\nu(P)\rbrace \\[3pt]&= 2\sup\limits_{A\subset\mathcal{X}}|\mu(A)-\nu(A)|.
\end{align*} 
\end{proof}
\noindent{}It is obvious that for any $\mu\in\mathcal{P(X)}$, $||\mu||_{TV}$ is bounded by 1. Unlike we did with the Wasserstein distance, we don't need to restrict the space of probability measures for Total Variation to be bounded.
\subsubsection{Theorem}
Total Variation is a metric on $\mathcal{P(X)}$.
\begin{proof}
The proof of this is analogous to the proof of theorem 4.1.2.
\end{proof}
\noindent{}The upper identity of the Total Variation distance can be used to prove a bounding property of the Wasserstein metric.
\subsection{Topological properties of Total Variation}
\subsubsection{Theorem}
Let $\mu$ and $\nu$ be two probability measures on a Polish space  $(\mathcal{X},d)$ and $p\in[1,\infty)$. Then it holds for an arbitrary $x_0\in\mathcal{X}$
\[
W_p(\mu,\nu)\leq{}2^\frac{1}{p'}\left(\int\limits_{\mathcal{X}}d(x_0,x)^pd|\mu-\nu|(x)\right)^\frac{1}{p},\quad\frac{1}{p}+\frac{1}{p'}=1.
\]
\begin{proof}
We show this by contriving a coupling of $(\mu,\nu)$ such that the upper inequality is satisfied.\vspace{2em}\\
We know that $\mu-\nu$ is a signed measure and there is a Hahn decomposition $P$ and $N$ of $(\mu-\nu)$. This means
\[
(\mu-\nu)^{+}(A) = \mu(A\cap{}P)-\nu(A\cap{}P)\quad\text{and}\quad (\mu-\nu)^{-}(A) = -\mu(A\cap{}N)+\nu(A\cap{}N).
\]
Then, of course it holds $(\mu-\nu)^{\pm}\geq{}0$.\vspace{1em}\\We also know that $a =: (\mu-\nu)^{+}(\mathcal{X})=(\mu-\nu)^{-}(\mathcal{X})$, since $(\mu-\nu)(\mathcal{X})=0$.\vspace{1em}\\
Now define $(\mu\wedge\nu):=\mu-(\mu-\nu)^{+}$. Take $\pi_1=(\mu\wedge\nu)\circ{}(\text{Id,Id})^{-1}$, defined by
\begin{align*}
\pi_1(A\times{}B)&=(\mu\wedge\nu)\circ{}(\text{Id,Id})^{-1}(\lbrace(x,y)\in\mathcal{X\times{}X}\,|\,x\in{}A,y\in{}B\rbrace) \\&= (\mu\wedge\nu)(\lbrace{}x\in\mathcal{X}\,|\,x\in{}A,x\in{}B\rbrace) \\&= (\mu\wedge\nu)(A\cap{}B) \\&= \mu(A\cap{}B) - (\mu-\nu)^{+}(A\cap{}B).
\end{align*}
Take $\pi_2:=a^{-1}(\mu-\nu)^{+}\otimes{}(\mu-\nu)^{-}$, which simply denotes the product measure of the Jordan decomposition. \vspace{1em}\\
Now $\pi:=\pi_1+\pi_2$ is obviously a probability measure on $\mathcal{X\times{}X}$, since
\begin{align*}
\mu(\mathcal{X\times{}X}) &= \pi_1(\mathcal{X\times{}X})+\pi_2(\mathcal{X\times{}X}) = \mu(\mathcal{X})-a+\frac{1}{a}
a^2 \\&= \mu(\mathcal{X})=1.
\end{align*}
The $\sigma$-additivity follows from the fact that $(\mu-\nu)^{\pm}$ are $\sigma$-additiv.\\
In fact, $\pi$ is a coupling of $(\mu,\nu)$, since it satisfies the marginal conditions. Let $A,B\subset\mathcal{X}$ be measurable sets, then it holds
\begin{align*}
\pi(A\times\mathcal{X})&=\pi_1(A\times\mathcal{X})+\pi_2(A\times\mathcal{X})\\&=\mu(A\cap\mathcal{X})-(\mu-\nu)^{+}(A\cap\mathcal{X})+a^{-1}(\mu-\nu)^{+}(A)(\mu-\nu)^{-}(\mathcal{X})\\&=\mu(A)-\mu(A\cap{}P)+\nu(A\cap{}P)+a^{-1}a(\mu-\nu)(A\cap{}P)\\&=\mu(A)-\mu(A\cap{}P)+\nu(A\cap{}P)+\mu(A\cap{}P)-\nu(A\cap{}P)\\&=\mu(A)
\end{align*}
and 
\begin{align*}
\pi(\mathcal{X}\times{}B)&=\pi_1(\mathcal{X}\times{}B)+\pi_2(\mathcal{X}\times{}B)\\&=\mu(\mathcal{X}\cap{}B)-(\mu-\nu)^{+}(\mathcal{X}\cap{}B)+a^{-1}(\mu-\nu)^{+}(\mathcal{X})(\mu-\nu)^{-}(B)\\&=\mu(B)-\mu(B\cap{}P)+\nu(B\cap{}P)+a^{-1}a(\nu-\mu)(B\cap{}N)\\&=\mu(B)-\mu(B\cap{}P)+\nu(B\cap{}P)+\nu(B\cap{}N)-\mu(B\cap{}N)\\&=\nu(B).
\end{align*}
Therefore $\pi$ denotes a transference plan between $\mu$ and $\nu$. We conclude 
\[
W_p(\mu,\nu)^p\leq\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi(x,y)=\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_1(x,y)+\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_2(x,y).
\]
Since $\pi_1=(\mu\wedge\nu)\circ{}(\text{Id,Id})^{-1}$ it is $\pi_1(A\times{}B)=(\mu\wedge\nu)(A\cap{}B)$ and therefore $\pi_1$ is concentrated on the diagonal. This means it holds
\[
\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_1(x,y)=0.
\]
Recall our arbitrarily given $x_0\in\mathcal{X}$. We are now left with
\[
\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_2(x,y)=\frac{1}{a}\int\limits_{\mathcal{X}}\int\limits_{\mathcal{X}}d(x,y)^pd(\mu-\nu)_{+}(x)d(\mu-\nu)_{-}(y).
\]
Using Jensen's inequality $(A+B)^p\leq{}2^{p-1}(A^p+B^p)$ on $(d(x,x_0)+d(y,x_0))^p$ yields
\begin{align*}
&\hspace{1em}\int\limits_{\mathcal{X\times{}X}}d(x,y)^pd\pi_2(x,y)\\&\leq\frac{1}{a}\int\limits_{\mathcal{X}}\int\limits_{\mathcal{X}}(d(x,x_0)+d(y,x_0))^pd(\mu-\nu)_{+}(x)d(\mu-\nu)_{-}(y)\\&\leq\frac{2^{p-1}}{a}\int\limits_{\mathcal{X}}\int\limits_{\mathcal{X}}d(x,x_0)^p+d(y,x_0)^pd(\mu-\nu)_{+}(x)d(\mu-\nu)_{-}(y)\\&=\frac{2^{p-1}}{a}\left(\int\limits_{\mathcal{X}}\int\limits_{\mathcal{X}}d(x,x_0)^pd(\mu-\nu)_{+}(x)d(\mu-\nu)_{-}(y)+\int\limits_{\mathcal{X}}\int\limits_{\mathcal{X}}d(y,x_0)^pd(\mu-\nu)_{+}(x)d(\mu-\nu)_{-}(y)\right)\\&=\frac{2^{p-1}}{a}\left(a\int\limits_{\mathcal{X}}d(x,x_0)^pd(\mu-\nu)_{+}(x)+a\int\limits_{\mathcal{X}}d(y,x_0)^pd(\mu-\nu)_{-}(y)\right)\\&=2^{p-1}\left(\int\limits_{\mathcal{X}}d(x,x_0)^pd(\mu-\nu)_{+}(x)+\int\limits_{\mathcal{X}}d(y,x_0)^pd(\mu-\nu)_{-}(y)\right)\\&=2^{p-1}\int\limits_{\mathcal{X}}d(x,x_0)^pd|\mu-\nu|(x).\hspace{27em}
\end{align*}
The last equality follows from the fact that $|\mu-\nu| = (\mu-\nu)^{+}+(\mu-\nu)^{-}.$ Note, that taking the $p$-th square root yields the factor $2^\frac{p-1}{p}$ and of course we know it holds $\frac{p-1}{p}+\frac{1}{p}=1$. Thus, choosing $p':=\frac{p}{p-1}$ yields the desired inequality.
\end{proof}
\subsubsection{Corollary}
Let $(\mathcal{X},d)$ be a metric space. If the diameter of $\mathcal{X}$ is bounded by $D$, then we obtain 
\begin{align*}
W_p(\mu,\nu)^p\leq{}2^{p-1}D^p\underbrace{|\mu-\nu|(\mathcal{X})}_{\|\mu-\nu\|_{TV}} = 2^{p-1}D^p\,\|\mu-\nu\|_{TV}
\end{align*}
and
\begin{align*}
W_p(\mu,\nu)\leq{}2^{\frac{p-1}{p}}D\sqrt[p]{\|\mu-\nu\|_{TV}}.
\end{align*}
In particular, we know 
\begin{align*}
W_1\leq\text{diam}(\mathcal{X})\|\cdot\|_{TV}.
\end{align*}
This means for a bounded Polish space $(\mathcal{X},d)$, the topology induced by $\|\cdot\|_{TV}$ is greater than the topology induced by $W_p$ on the Wasserstein space $P_p(\mathcal{X})$. Knowing that a bounded Polish space implies $P_p(\mathcal{X})=\mathcal{P(X)}$ and that the topology on $\mathcal{P(X)}$ is simply the topology of regular weak convergence, we now conclude that Total Variation induces a topology greater than that of weak convergence on $\mathcal{P(X)}$, if $(X,d)$ is a bounded Polish space.
\subsubsection{Corollary}
Let $(\mathcal{X},d)$ be a bounded Polish space and $(\mu_n)_{n\in\mathbb{N}}\subset\mathcal{P(X)}$ a sequence. Given another probability measure $\mu\in\mathcal{X}$, it holds 
\begin{align*}
\|\mu_n-\mu\|_{TV}\longrightarrow{}0\qquad \Longrightarrow \qquad \mu_n\,\substack{\omega \\ \longrightarrow}\,\mu.
\end{align*}
There is also another inversed inequality showing that $W_p$ is an upper bound of $\|\cdot\|_{TV}$. 
\subsubsection{Theorem}
Let $(\mathcal{X},d)$ be a finite metric space. Then it holds 
\[
W_p\geq{}d_{\text{min}}\sqrt[p]{\|\cdot{}\|_{TV}},
\]
with $d_{\text{min}}:=\min\limits_{x\neq{}y}d(x,y)$.\\ 
This of course implies as a corollary $W_1\geq{}d_{\text{min}}\|\cdot\|_{TV}.$
\begin{proof}
The inequality follows from the fact that it holds $d(x,y)\geq{}d_{\text{min}}1_{[x\neq{}y]}(x,y)$ for all $x,y\in\mathcal{X}$, which yields
\[
W_p(\mu,\nu)^p=\inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X}}d(x,y)^pd\pi(x,y)\geq \inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X}}d_{\text{min}}^p\cdot{}1_{[x\neq{}y]}(x,y)d\pi(x,y) = d_{\text{min}}^p\|\cdot{}\|_{TV}.
\]
\end{proof}
\noindent{}Note that for the second inequality to hold, finiteness was needed. Otherwise $d_{\text{min}}$ could have been 0. On an infinite set $\mathcal{X}$ one could potentially define a sequence $(x_n)_{n\in\mathbb{N}}$ that converges to some $x$. Then it would hold for the sequence $\delta_{x_n}$ 
\[
W_p(\delta_{x_n},\delta_x) = d(x_n,x)\longrightarrow 0,
\]
but also
\[
||\delta_{x_n}-\delta_x{}||_{TV} = 1_{[x\neq{}y]}(x_n,x) = 1.
\]
Recall that the Wasserstein distance metrized weak convergence on the Wasserstein space $P_p(\mathcal{X})$. What we have proven now, is that, given a bounded Polish space, Total Variation bounds the Wasserstein distance from above, thus metrizing a topology greater than that of weak convergence. If, additionally the Polish space is finite then the Wasserstein distance bounds Total Variation from above, creating a topology greater than the one metrized by Total Variation. Hence we know that on a finite Polish space Total Variation metrizes weak convergence. (Here the boundedness can be neglected, since finite implies bounded). Note here, that given a finite Polish space the Wasserstein space $P_p(\mathcal{X})$ is simply given by the space of probability measures $\mathcal{P(X)}$.\vspace{3em}\\
Another interesting approach to the Total Variation metric is taking the metric space $(\mathcal{X},d)$, where $d=1_{[x\neq{}y]}$ is the discrete metric. The topology induced by the discrete metric is the power set of $\mathcal{X}$, since for every $x\in\mathcal{X}$ the set $\lbrace{}x\rbrace$ is open. Then the Borel $\sigma$-algebra on $\mathcal{X}$ is given by the power set too. It is obvious that $(\mathcal{X},d)$ is complete. $(\mathcal{X},d)$ is separable if and only if $\mathcal{X}$ is countable. The Wasserstein distance on $(\mathcal{X},d)$ is now given by
\begin{align*}
W_p(\mu,\nu)^p=\inf\limits_{\pi\in\Pi(\mu,\nu)}\int\limits_{\mathcal{X\times{}X}}1_{[x\neq{}y]}(x,y)d\pi(x,y) = \|\mu-\nu\|_{TV}.
\end{align*} 
Now recall that the Wasserstein space was given by 
\begin{align*}
\bigg\lbrace\mu\in\mathcal{P(X)}\,;\,\int\limits_{\mathcal{X}}1^p_{[x\neq{}x_0]}(x)d\mu(x)<\infty\bigg\rbrace.
\end{align*}
It is of course 
\begin{align*}
\int\limits_{\mathcal{X}}1^p_{[x\neq{}x_0]}(x)d\mu(x)\leq\mu(\mathcal{X})=1.
\end{align*}
Hence $P_p(\mathcal{X}) = \mathcal{P(X)}$. At this point it is important to note, that given the discrete metric, $\mathcal{P(X)}$ denotes the set of all probability measures, defined on the power set of $\mathcal{X}$, which is a very limited set. \vspace{1em}\\
We recall that given a Polish space $(\mathcal{X},d)$ the Wasserstein distance metrizes weak convergence on $P_p(\mathcal{X})$, which endowed with the Wasserstein distance is also a Polish space. \vspace{1em}\\
We now also know that on this particular metric space, the Total Variation distance and the Wasserstein distance are the same, thus inducing the same topology on $\mathcal{P(X)}$.
Given a countable set $\mathcal{X}$ endowed with the discrete metric, the set of all probability measures $\mathcal{P(X)}$ defined on the power set of $\mathcal{X}$, endowed with the topology of regular weak convergence is a Polish space. Furthermore if $(\mathcal{X},d)$ is compact, which for $d=1_{[x\neq{}y]}$ is equivalent to $\mathcal{X}$ finite, then $\mathcal{P(X)}$ again, endowed with the topology of weak convergence is compact, which by Prokhorov's theorem, implies $\mathcal{P(X)}$ is tight.\vspace{1em}\\
To recap:\\
Given a finite set $\mathcal{X}$ the set of all probability measures, that are defined on every subset of $\mathcal{X}$ is a compact (and tight) Polish space. For $\mathcal{X}$ countable, it is only a Polish space. \vspace{2em}\\  
This fairly well sums up the Total Variation distance. We move on to introducing the reader to another probability metric, namely the Prokhorov metric.
\section{The Prokhorov metric}
Let $(\mathcal{X},d)$ be a metric space.\vspace{1em}\\
Take two probability measures $\mu,\nu\in\mathcal{P(X)}$. Then the Prokhorov metric between $\mu$ and $\nu$ is defined as
\[
d_P(\mu,\nu):=\inf\lbrace\epsilon{}>0\,;\,\mu(B)\leq\nu(B^\epsilon)+\epsilon \,\,\text{for all Borel sets}\,\,B\subset\mathcal{X}\rbrace,
\]
where we define $B^\epsilon = \lbrace{}x\in\mathcal{X};\inf\limits_{y\in{}B}d(x,y)\leq\epsilon\rbrace.$\vspace{1em}\\
\subsubsection{Theorem}
The Prokhorov metric satisfies all axioms of a metric
\begin{proof}
We start off, by proving the symmetry $d_P(\mu,\nu)=d_P(\nu,\mu)$. \\Let $\varepsilon:=d_P(\mu,\nu)$ and take an arbitrary Borel set $A_1\subset\mathcal{X}$. We define $A_2:=\left(A_1^{\varepsilon}\right)^{c}$. It is $A_1\subset\left(A_2^{\varepsilon}\right)^{c}$ from which we conclude
\begin{align*}
\mu(A_1^{\epsilon}) = 1-\mu(A_2)\geq{}1-\nu(A_2^{\epsilon})-\epsilon=\nu\left(\left(A_2^{\varepsilon}\right)^{c}\right)-\epsilon\geq\nu(A_1)-\epsilon
\end{align*}
and consequently every Borel set $A\subset\mathcal{X}$ satisfies the inequality
\begin{align*}
\nu(A)\leq\mu(A^\epsilon)+\epsilon.
\end{align*}
Note that $B^0=\overline{B}$ and $d_P(\mu,\nu)=0$ then implies 
\begin{align*}
\mu(A)\leq\nu(\overline{A})\quad\text{and}\quad\nu(A)\leq\mu(\overline{A}).
\end{align*}
This yields $\mu(A)=\nu(A)$ for all closed sets $A\subset\mathcal{X}$. From the uniqueness of measures it follows that $\mu=\nu$ on $\mathcal{X}$. The other implication is obvious.\vspace{1em}\\
For the triangle inequality, take three arbitrary probability measures $\mu,\nu,\lambda\in\mathcal{P(X)}$ and define $\epsilon:=d_P(\mu,\lambda)$ and $\epsilon':=d_P(\nu,\lambda).$ \vspace{1em}\\Then it holds for $A\subset\mathcal{X}$ measurable
\[
\mu(A)\leq\lambda(A^{\epsilon})+\epsilon\leq\nu(A^{\epsilon^{\epsilon'}})+\epsilon+\epsilon'\leq\nu(A^{\epsilon+\epsilon'})+\epsilon+\epsilon',
\]
and therefore $d_P(\mu,\nu)\leq{}d_P(\mu,\lambda)+d_P(\lambda,\nu).$
\end{proof}
\noindent{}We know now that $d_P$ is, in fact a metric on $\mathcal{P(X)}$, which assumes values in $[0,1]$. While not easy to compute this metric is theoretically important, because it metrizes weak convergence.
\subsection{Topological properties of the Prokhorov metric}
\subsubsection{Theorem}
For a sequence of probability measures $(\mu_n)_{n\in\mathbb{N}}\subset\mathcal{P(X)}$ and $\mu\in\mathcal{P(X)}$ it holds
\[
d_P(\mu_n,\mu)\longrightarrow{}0\quad\quad\Longrightarrow\quad\quad\mu_n\,\,\substack{\omega \\ \xrightarrow{\hspace{1.2em}}}\,\,\mu.
\]
\begin{proof}
This is easily proven, using Portmanteau's theorem. We will show that for every closed set $A\subset\mathcal{X}$ it holds
\[
\limsup\limits_{n\rightarrow\infty}\mu_n(A)\leq\mu(A).
\]
Since $d_P(\mu_n,\mu)\longrightarrow{}0$, we know that 
\[
\forall\varepsilon>0\,\exists{}N\in\mathbb{N}\,\forall{}n\in\mathbb{N}_{\geq{}N} : \mu_n(A)\leq\mu(A^{\varepsilon})+\varepsilon.
\]
Letting $\varepsilon\longrightarrow{}0$ yields 
\[
\limsup\limits_{n\rightarrow\infty}\mu_n(A)\leq\mu(A).
\]
This has shown $\mu_n\,\,\substack{\omega\\ \xrightarrow{\hspace{1.2em}}}\,\,\mu.$ 
\end{proof}
\noindent{}We have now shown that for any metric set, the Prokhorov metric metrizes a topology greater than that of weak convergence. We now show that given an additional restriction, it will also induce a topology smaller than that of weak convergence, thereby metrizing weak convergence.
\subsubsection{Theorem}
Let $(\mathcal{X},d)$ be a separable metric space. Then for a sequence of probability measures $(\mu_n)_{n\in\mathbb{N}}\subset\mathcal{P(X)}$ and some $\mu\in\mathcal{P(X)}$ it holds 
\[
\mu_n\,\,\substack{\omega \\ \xrightarrow{\hspace{1.2em}}}\,\,\mu \quad\quad\Longrightarrow\quad\quad d_P(\mu_n,\mu)\longrightarrow{}0.
\]
The proof of this is not quite as trivial as the upper one.
\begin{proof}
We know that for any open set $B\subset\mathcal{X}$ it holds $\liminf\limits_{n\rightarrow\infty}\mu_n(B)\geq\mu(B)$ and subsequently
\begin{align*}
\forall\epsilon>0\,\exists{}N\in\mathbb{N}\,\forall{}n\in\mathbb{N}_{\geq{}N}:\,\mu_n(B)\geq\mu(B)-\epsilon.
\end{align*}
With this in mind we will try to prove that the upper inequality holds for any measurable set $A$ and then conclude the convergence in $d_p$.
Let $(A_n)_{n\in\mathbb{N}}$ be a partition of $\mathcal{X}$, i.e a sequence of pairwise disjoint subsets that cover $\mathcal{X}$. \vspace{1em}\\Additionally we impose that the diameter of these sets must not exceed $\epsilon$, i.e \[\sup\limits_{x,y\in{}A_n}d(x,y)\leq\epsilon\quad\quad\text{f.a. }n\in\mathbb{N}.\] Note, that the existence of a partition like this follows directly from the fact that any separable metric space is second-countable, i.e its topology has a countable base.\vspace{1em}\\
We know that it holds
\[
\mu\left(\dot{\bigcup\limits_{n\in\mathbb{N}}}A_n\right)=1.
\]
This, of course means there exists some $k\in\mathbb{N}$ such that 
\[
\mu\left(\dot{\bigcup\limits_{n>k}}A_n\right)<\epsilon.
\]
We define $\mathcal{G}$ as the set of all sets of the form
\[
\left(\left(A_{i_1}\cup\ldots\cup{}A_{i_m}\right)^{\epsilon}\right)^{\circ}\quad\text{with}\,\,\,\,1\leq{}i_1,\ldots{},i_m\leq{}k.
\]
It should be added, that we define $\left(A^{\epsilon}\right)^{\circ}:=\lbrace{}x\in\mathcal{X};\,d(x,A)<\epsilon\rbrace$. Note that $\mathcal{G}$ then only consists of open sets. Using weak convergence of $(\mu_n)_{n\in\mathbb{N}}$, we know that for every $n\geq{}N$ and any $G\in\mathcal{G}$ it holds 
\[
\mu_n(G)\geq\mu(G)-\epsilon.
\]
Finally we can go on to prove convergence in the topology, induced by $d_P$.\\
Take an arbitrary $A\subset\mathcal{X}$ and define $A_0$ as the union of all sets $A_i$ $(i\leq{}k)$, that overlap with $A$.\vspace{1em}\\
It is obvious that $\left(A_0^{\epsilon}\right)^{\circ}\in\mathcal{G}$ and therefore it holds $\mu(\left(A_0^{\epsilon}\right)^{\circ})\leq\mu_n(A_0^{\epsilon})+\epsilon$. Then it holds for all $n\geq{}N$
\begin{align*}
\mu(A) &= \mu\left(A\,\,\cap\,\,\dot{\bigcup\limits_{n\in\mathbb{N}}}A_n\right) = \mu(A\cap{}A_0) + \mu\left(A\,\,\cap\,\,\dot{\bigcup\limits_{n>k}}A_n\right) \\&\leq \mu(\left(A_0^{\epsilon}\right)^{\circ}) + \mu\left(\dot{\bigcup\limits_{n>k}}A_n\right) \leq\,\mu(\left(A_0^{\epsilon}\right)^{\circ})+\epsilon \\[5pt]&\leq\mu_n(\left(A_0^{\epsilon})\right)^{\circ})+\epsilon{}+\epsilon \,\,\leq\,\,\mu_n(A_0^{\epsilon})+\epsilon+\epsilon\\[10pt]&\leq \mu_n(A^{2\epsilon})+2\epsilon.
\vspace{0.3em}\\
\end{align*}
We know now that for any $\epsilon>0$ there exists $N\in\mathbb{N}$ sucht that for all $n\geq{}N$ it holds
\[
d_P(\mu_n,\mu)\leq{}2\epsilon.
\]
Hence, we conclude 
\[
d_P(\mu_n,\mu)\longrightarrow{}0.
\]
\end{proof}
\noindent{}We have now shown that for a separable metric space $(\mathcal{X},d)$ the Prokhorov metric metrizes weak convergence on $\mathcal{P(X)}$. Recall the theorem, stating that if $(\mathcal{X},d)$ is a Polish space, then $\left(P_p(\mathcal{X}),W_p\right)$ is Polish as well. We will show the same for the Prokhorov metric. 
\subsubsection{Theorem}
Let $(\mathcal{X},d)$ be a Polish space, then $(\mathcal{P(X)},d_P)$ is a Polish space as well.
\begin{proof}
We start off by showing the separability of $(\mathcal{P(X)},d_P)$.\vspace{1em}\\
Let $\varepsilon>0$ be arbitrary. As we did in the proof above, we take a partition of $(A_n)_{n\in\mathbb{N}}$ of $\mathcal{X}$ such that $A_n\cap{}A_m=\varnothing\,\,\,(n\neq{}m)$, with $\text{diam}(A_n)\leq\varepsilon$ and we take a sequence of points $(x_n)_{n\in\mathbb{N}}\subset\mathcal{X}$ such that $x_n\in{}A_n$.
Define
\[
D:=\Big\lbrace\sum\limits_{i=1}^{n}r_i\delta_{x_i};\,\,n\in\mathbb{N},r_i\in\mathbb{Q},x_i\in{}A_i\Big\rbrace.
\] 
We will show that $\mathcal{D}$ is dense in $(\mathcal{P(X)},d_P)$ and because it is obviously countable, this then yields the separability. We now take an arbitrary $\mu\in\mathcal{P(X)}$ and show that there exists a measure $\lambda\in\mathcal{D}\,$ with $\,d_P(\mu,\lambda)\leq\varepsilon$. \vspace{1em}\\Since $\bigcup_{n\in\mathbb{N}}A_n=\mathcal{X}$ we know that there exists a $k\in\mathbb{N}$ such that 
\[
\mu\left(\bigcup\limits_{i>k}A_i\right)<\varepsilon.
\]
We now choose $r_1,\ldots,r_k\in\mathbb{Q}$ such that 
\[
\sum\limits_{i=1}^{k}r_i=1\quad\text{and}\quad\sum\limits_{i=1}^{k}|r_i - \mu(A_i)|<\varepsilon.
\]
This of course is feasible, because $\mathbb{Q}$ is dense in $\mathbb{R}.$ Now define 
\[
\nu=\sum\limits_{i=1}^{k}r_i\delta_{x_i}.
\]
We show that for any measurable set $A\subset\mathcal{X}$ it holds $\mu(A)\leq\nu(A^{\varepsilon})+\varepsilon$. As we did before we take all $A_{i_1},\ldots,A_{i_m}\,\,\,(1\leq{}i_1,\ldots,i_m\leq{}k)$ with $A_{i_j}\cap{}A\neq\varnothing$ and define $A_0=\bigcup\limits_{j=1}^{m}A_{i_j}$. With the fact that the $A_i$ are disjoint we obtain
\begin{align*}
\mu(A)&=\mu(A\cap\bigcup\limits_{i\leq{}k}A_i)+\mu(A\cap\bigcup\limits_{i>k}A_i)\leq\mu(A\cap{}A_0)+\varepsilon \\&\leq \sum\limits_{j=1}^{m}\mu(A_{i_j}) + \varepsilon\leq\sum\limits_{j=1}^{m}r_{i_j}+\varepsilon+\varepsilon=\nu(A_0)+2\varepsilon\leq\nu(A^{\varepsilon})+2\varepsilon \\[5pt]&\leq\nu(A^{2\varepsilon})+2\varepsilon.
\end{align*}
Hence we know that for any $\varepsilon>0$ and any measurable $A\subset\mathcal{X}$ it holds 
\[
\mu(A)\leq\nu(A^{2\varepsilon})+2\varepsilon,
\]
which means $d_P(\mu,\nu)\leq{}2\varepsilon$. Therefore we know that for any $\mu\in\mathcal{P(X)}$ and any $\varepsilon>0$ there exists $\nu\in\mathcal{D}$ such that $d_P(\mu,\nu)\leq{}\varepsilon$. Consequently $\mathcal{D}$ is dense in $\mathcal{P(X)}$. This concludes the proof that $(\mathcal{P(X)},d_P)$ is separable. Note that we didn't use the completeness of $(\mathcal{X},d)$ in this part ,which means the separability of $(\mathcal{X},d)$ directly implies the separability of $(\mathcal{P(X)},d_P).$ \vspace{1em}\\
For the completeness, let $(\mu_n)_{n\in\mathbb{N}}$ be a $d_P$-Cauchy sequence. We will show that $(\mu_n)_{n\in\mathbb{N}}$ is tight. Tightness will then imply that there exists a convergent subsequence via Prokhorov's theorem, through which we will obtain convergence of $(\mu_n)_{n\in\mathbb{N}}$.\vspace{1em}\\
Take an arbitary $m\in\mathbb{N}$ and $\varepsilon>0$. Knowing that $(\mu_n)_{n\in\mathbb{N}}$ is a $d_P$-Cauchy sequence, we can find find $N_m\in\mathbb{N}$ such that for all $n\in\mathbb{N}_{>N_m}$ it holds
\[
d_P(\mu_n,\mu_{N_m})\leq{}2^{-m}\varepsilon.
\]
Given the fact that on a Polish space every finite set of probability measures is tight we can find a compact set $K\subset\mathcal{X}$ such that 
\[
\sup\limits_{j\leq{}N_m}\mu_j(K^c)\leq{}2^{-m}\varepsilon.
\]
By compactness we know that for every $m\in\mathbb{N}$ $K$ can be covered by finitely many balls of radius $2^{-m}\varepsilon$, i.e for all $m\in\mathbb{N}$, there exists $\tilde{N}_m\in\mathbb{N}$ such that
\[
K\subset\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m}\varepsilon).
\]
The fact that it holds $d_P(\mu_{N_m},\mu_n)\leq{}2^{-m}\varepsilon\,\,$ yields for all $n\in\mathbb{N}_{\geq{N_m}}$
\begin{align*}
& \mu_n\left(\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m+1}\varepsilon)\right) \geq \mu_n\left(\left(\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m}\varepsilon)\right)^{2^{-m}\varepsilon}\right) \\[4pt]& \geq \mu_N\left(\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m}\varepsilon)\right)-2^{-m}\varepsilon \\[8pt] &\geq 1-2^{-m}\varepsilon-2^{-m}\varepsilon \\[10pt]& = 1-2^{-m+1}\varepsilon.
\end{align*}
Therefore it holds for all $n\in\mathbb{N}$
\begin{align*}
\mu_n\left(\left(\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m+1}\varepsilon)\right)^c\,\,\right) \leq 2^{-m+1}\varepsilon.
\end{align*}
The problem is that 
\begin{align*}
\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m+1}\varepsilon)
\end{align*}
isn't compact. Define now 
\[
C:=\bigcap\limits_{m\in\mathbb{N}}\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m+1}\varepsilon).
\]
Then $C$ is closed and totally bounded and therefore compact. It holds for $n\in\mathbb{N}$ arbitrary
\[
\mu_n(C^c)\leq\sum\limits_{m\in\mathbb{N}} \mu_n\left(\left(\bigcup\limits_{i=1}^{\tilde{N}_m}\overline{B}(x_i,2^{-m+1}\varepsilon)\right)^c\,\,\right) \leq \sum\limits_{m\in\mathbb{N}}2^{-m+1}\varepsilon = 2\varepsilon.
\]
To recap, we know that for every $\varepsilon>0$ there exists $C\subset\mathcal{X}$ compact, such that 
\[
\sup\limits_{n\in\mathbb{N}}\mu_n(C^c)\leq{}2\varepsilon.
\]
Therefore $(\mu_n)_{n\in\mathbb{N}}$ is tight and by Prokhorov's theorem, there exists a convergent subsequence of $(\mu_n)_{n\in\mathbb{N}}$. Knowing that a Cauchy sequence with convergent subsequence is already convergent itself, this directly implies the convergence of $(\mu_n)_{n\in\mathbb{N}}$. Thus $(\mathcal{P(X)},d_P)$ is complete. 
\end{proof}
\noindent{}An interesting aspect of this proof, was the fact that every $d_P$-Cauchy sequence is tight, analogously to Lemma 5.1.2.\vspace{1em}\\
Recall that for $(\mathcal{X},d)$ Polish, the Wasserstein metric metrized weak covergence on $P_p(\mathcal{X})$. For a bounded metric $d$, we also knew that the Wasserstein space $P_p(\mathcal{X})$ was equal to the space of all probability measures $\mathcal{P(X)}$. This means that since both, Wasserstein and Prokhorov metrize weak convergence they must be equivalent, at least for a bounded Polish space. That means there must exist some kind of lower and upper bound of Wasserstein by Prokhorov. We introduce a Lemma with which we will show exactly this. 
\subsubsection{Lemma}
Let $(\mathcal{X},d)$ be a separable metric space and $\mu,\nu\in\mathcal{P(X)}$ be two probability measures, such that $d_P(\mu,\nu)<\alpha$ for some $\alpha\in\mathbb{R}_{+}$.\vspace{1em}\\ Then there exists some probability measure $\pi$ on $\mathcal{X\times{}X}$ with marginals $\mu$ and $\nu$ (i.e. a coupling of $(\mu,\nu)$) such that 
\[
\pi\left((x,y)\in\mathcal{X\times{}X}\,|\,d(x,y)\geq\alpha\right)\leq\alpha.
\] 
The proof of this Lemma is quite long and complicated. It can be found in \cite{Billingsley}.\vspace{1em}\\
\noindent{}Another important Lemma needed, in order to prove the bounding of the Wasserstein distance by the Prokhorov metric is Markov's inequality.
\subsubsection{Lemma}
Let $X$ be a nonnegative random variable on a measurable space $(\Omega,\mathbb{P})$ and $c>0$ then it holds
\begin{align*}
\mathbb{P}(X\geq{}c)\leq\frac{\mathbb{E}[X]}{c}.\vspace{2em}\\
\end{align*}
\subsection{Bounding properties of the Prokhorov metric}
This brings us to the main point, the bounding properties of the Prokhorov metric and the Wasserstrein metric.
\subsubsection{Theorem}
Let $(\mathcal{X},d)$ be a bounded Polish space then the Wasserstein metric of order 1 and Lévy-Prokhorov satisfy
\begin{align*}
(d_P)^2 \leq W_1 \leq (\text{diam}(\mathcal{X})+1)d_P.
\end{align*}
\begin{proof}
We start with the right-hand side. Choose $\varepsilon>0$ such that $d_P(\mu,\nu)<\varepsilon$ and choose a coupling $\pi$ of $(\mu,\nu)$ such that 
\begin{align*}
\pi\left((x,y)\in\mathcal{X\times{}X}\,|\,d(x,y)\geq\varepsilon\right)\leq\varepsilon,
\end{align*}
which exists according to lemma 8.1.4. We obtain\vspace{0.25em}\\
\begin{align*}
\int\limits_{\mathcal{X\times{}X}}d(x,y)d\pi(x,y)\, &\leq \,\varepsilon\pi\left(d(x,y)\leq\varepsilon\right) + \text{diam}(\mathcal{X})\,\pi\left(d(x,y)>\varepsilon\right) \\[-15pt]&= \varepsilon + (\text{diam}(\mathcal{X})-\varepsilon)\cdot\pi(d(x,y)>\varepsilon)\\&\leq\varepsilon + (\text{diam}(\mathcal{X})-\varepsilon)\varepsilon \\&\leq(\text{diam}(\mathcal{X})+1)\varepsilon.
\end{align*}
Taking the infimum over all $\varepsilon$ on the right-hand side such that $d_P(\mu,\nu)\leq\varepsilon$ yields
\begin{align*}
W_1\leq{}(\text{diam}(\mathcal{X})+1)d_P.
\end{align*}
Now to the left inequality. We set $\varepsilon{}:=\sqrt{W_1(\mu,\nu)}$. Then Markov's inequality over the optimal coupling of $(\mu,\nu)$ yields
\begin{align*}
\pi(d(x,y)\geq\varepsilon) \leq \frac{1}{\varepsilon}\int\limits_{\mathcal{X\times{}X}}d(x,y)d\pi(x,y)\leq\frac{1}{\varepsilon}\varepsilon^2=\varepsilon.
\end{align*}
Knowing this, we can conclude that for any Borel set $B$ it holds
\begin{align*}
\mu(B) &= \pi(B\times\mathcal{X}) \leq \pi(B\times{}B^{\varepsilon}) + \pi(B\times{}\lbrace{}y\in\mathcal{X};\,\inf\limits_{x\in{}B}d(x,y)>\varepsilon\rbrace) \\&\leq \pi(B\times{}B^{\varepsilon}) + \pi(d(x,y)>\varepsilon)\\[5pt]&\leq\pi(\mathcal{X}\times{}B^{\varepsilon})+\varepsilon \\[4pt]&= \nu(B^{\varepsilon})+\varepsilon,
\end{align*}
which means $d_P(\mu,\nu)\leq\varepsilon$ and therefore $(d_P)^2\leq{}W_1.$
\end{proof}
\noindent{}We included something very interesting in the proof of the upper assertion. Recall lemma 8.1.4, where we stated, that if for two probability measures $\mu$ and $\nu$ it holds $d_P(\mu,\nu)<\alpha$, then we know that there exists a coupling $\pi$ of $(\mu,\nu)$ such that 
\[
\pi(d(x,y)\geq\alpha)\leq\alpha.
\]
This means that 
\begin{align*}
\inf\limits_{\pi\in\Pi(\mu,\nu)}\pi(d(x,y)>\geq\alpha)\leq{}d_P(\mu,\nu)\quad\text{ for all }\alpha>d_P(\mu,\nu),
\end{align*}
and therefore
\begin{align*}
\inf\lbrace{}\alpha>0;\,\,\inf\limits_{\pi\in\Pi(\mu,\nu)}\left(\pi(\mu,\nu)\geq\alpha\right)\leq\alpha\rbrace \leq d_P(\mu,\nu).
\end{align*}
Now we have shown in the upperhand proof, that if $\pi(d(x,y)\geq\alpha)\leq\alpha$, then it also holds $d_P(\mu,\nu)<\alpha$. 
Hence
\begin{align*}
d_P(\mu,\nu)\leq \inf\lbrace{}\alpha>0;\,\,\inf\limits_{\pi\in\Pi(\mu,\nu)}\left(\pi(\mu,\nu)\geq\alpha\right)\leq\alpha\rbrace.
\end{align*}
We obtain the equality
\begin{align*}
d_P(\mu,\nu)=\inf\lbrace\varepsilon{}>0;\,\,\inf\limits_{\pi\in\Pi(\mu,\nu)}\pi\left(d(x,y)>\varepsilon\right)\leq\varepsilon\rbrace.
\end{align*}
Before we move on to introducing the reader to another probability metric, we will examine another bounding property of Lévy-Prokhorov with Total Variation.\\
Recall that 
\begin{align*}
||\mu-\nu||_{TV}=\max\limits_{|\phi|\leq{}1}\int\limits_{\mathcal{X}}\phi(x)d(\mu-\nu)(x).
\end{align*}
\subsubsection{Theorem}
Let $(\mathcal{X},d)$ be a metric space and $\mu,\nu\in\mathcal{P(X)}$ two arbitrary probability measures. Then
\begin{align*}
d_P(\mu,\nu)\leq{}||\mu-\nu||_{TV}.
\end{align*}
\begin{proof}
Take $\varepsilon>0$ such that $||\mu-\nu||_{TV}\leq\varepsilon$, then we know that for every measurable function $\phi$ with $|\phi|\leq{}1$ it is 
\begin{align*}
\int\limits_{\mathcal{X}}\phi(x)d(\mu-\nu)(x)\leq\varepsilon,
\end{align*}
and therefore 
\begin{align*}
\int\limits_{\mathcal{X}}1_A(x)d\mu(x) - \int\limits_{\mathcal{X}}1_A(y)d\nu(y) = \mu(A)-\nu(A)\leq\varepsilon\quad\text{ for all measurable }A,B\subset\mathcal{X},
\end{align*}
which means $\mu(A)\leq\nu(A)+\varepsilon\leq\nu(A^{\varepsilon})+\varepsilon$. Consequently it holds $d_P(\mu,\nu)\leq\varepsilon$ and we conclude $d_P\leq{}\|\cdot\|_{TV}.$
\end{proof}
\section{Lévy metric}
The Levy metric is designed to be the equivalent of Prokhorov on $\mathbb{R}$ and is defined as follows:\vspace{1em}\\
Let $\mu$ and $\nu$ be two probability measures on $\mathbb{R}$ and $F$ and $G$ their respective distribution functions. Then 
\begin{align*}
d_L(\mu,\nu):=d_L(F,G):=\inf\lbrace\varepsilon>0\,:\,G(x-\varepsilon)\leq{}F(x)\leq{}G(x+\varepsilon), \forall{}x\in\mathbb{R}\rbrace.
\end{align*}
Recall that $F(x)=\mu((-\infty,x])$ and $G(x)=\nu((-\infty,x])$.
\subsubsection{Theorem}
The Lévy metric $d_L$ satisfies all axioms of a metric.
\begin{proof}
$d_L\geq{}0$ follows directly from the fact that we take the supremum over all $\varepsilon>0$.\\
$d_L(\mu,\nu)=0$ directly implies $F=G$ and therefore $\mu(-\infty,x]) = \nu((-\infty,x])$ for all $x\in\mathbb{R}$. We know that the set of all intervalls of the form $(-\infty,a]$ generates the Borel $\sigma$-algebra on $\mathbb{R}$. It satisfies the conditions of the uniqeness theorem of measures and since it holds $d_L(\mu,\nu)=0$, we know that $\mu=\nu$. The other implication is obvious.\vspace{1em}\\
The symmetry is fairly obvious too. Let $\varepsilon:=d_L(\mu,\nu)$. Then we know that 
\begin{align*}
\mu((-\infty,x-\varepsilon])-\varepsilon \leq \nu((-\infty,x]) \leq \mu((-\infty,x+\varepsilon])+\varepsilon,
\end{align*}  
and therefore it holds 
\begin{align*}
&\nu((-\infty,x-\varepsilon]) \leq \mu((-\infty,x-\varepsilon+\varepsilon])+\varepsilon \\ \text{ and }& \\&\nu((-\infty,x+\varepsilon]) \geq \mu((-\infty,x+\varepsilon-\varepsilon])-\varepsilon.
\end{align*}
Hence
\begin{align*}
\nu((-\infty,x-\varepsilon])-\varepsilon \leq \mu((-\infty,x]) \leq \nu((-\infty,x+\varepsilon])+\varepsilon,
\end{align*}
which means $\varepsilon=d_L(\nu,\mu)$, thereby yielding the symmetry.\vspace{1em}\\
For the triangle inequality, let's take three abritrary measures $\mu,\nu,\lambda\in\mathcal{P(X)}$ with $\varepsilon:=d_L(\mu,\lambda)$ and $\varepsilon':=d_L(\lambda,\nu)$ and therefore it holds
\begin{align*}
&\mu((-\infty,x-\varepsilon])-\varepsilon \leq \lambda((-\infty,x]) \leq \mu((-\infty,x+\varepsilon])+\varepsilon
\\\text{ and }& \\
&\lambda((-\infty,x-\varepsilon'])-\varepsilon' \leq \nu((-\infty,x]) \leq \lambda((-\infty,x+\varepsilon'])+\varepsilon'.
\end{align*}
We obtain
\begin{align*}
\mu((-\infty,x-\varepsilon-\varepsilon'])-\varepsilon-\varepsilon' \,&\leq \lambda((-\infty,x-\varepsilon'])-\varepsilon' \\[5pt]&\leq \nu((-\infty,x]) \\[5pt]&\leq \lambda((-\infty,x+\varepsilon'])+\varepsilon' \\[5pt]&\leq \mu((-\infty,x+\varepsilon'+\varepsilon])+\varepsilon'+\varepsilon
\end{align*}
and we deduce
\[
d_L(\mu,\nu)\leq\varepsilon'+\varepsilon.
\]
\end{proof}
\noindent{}We know that $d_L$ is a metric on $\mathcal{P}(\mathbb{R})$. Note that of course $(-\infty,x]$ is a measurable set in the Borel $\sigma$-algebra on $\mathbb{R}$ and it is $(-\infty,x]^{\epsilon} = (-\infty,x+\epsilon]).$ Thereupon $d_L$ is nothing but a weaker version of $d_P$ on $\mathbb{R}$. Ergo it holds
\begin{align*}
d_L\leq{}d_P.
\end{align*}
That bounding property justifies why the topology on $\mathcal{P}(\mathbb{R})$, induced by $d_L$ is smaller than that of weak convergence on $\mathbb{R}$, i.e for a sequence of probability measures $(\mu_n)_{n\in\mathbb{N}}$ and $\mu$ in $\mathcal{P}(\mathbb{R})$ it holds
\begin{align*}
\mu_n \,\xrightarrow{\omega}\, \mu \qquad \Longrightarrow \qquad d_L(\mu_n,\mu)\longrightarrow{}0.
\end{align*}
What's interesting about this is, that in addition to being bounded by $d_P$, $d_L$ metrizes weak convergence on $\mathbb{R}$.\vspace{1em}\\
\subsubsection{Theorem}
Let $(\mu_n)_{n\in\mathbb{N}}\subset\mathcal{P}(\mathbb{R})$ be a sequence with $d_L(\mu_n,\mu)\longrightarrow{}0$ for some $\mu\in\mathcal{P}(\mathbb{R})$. \vspace{1em}\\Then it holds \quad $\mu_n\,\xrightarrow{\,\,\,\omega\,\,\,}\mu$. 
\begin{proof}
We know that 
\begin{align*}
&\quad\forall\varepsilon>0\,\exists{}N\in\mathbb{N}\,\forall{}n\geq{}N\,:\,d_L(\mu_n,\mu)<\varepsilon\\\text{i.e}&\\&\quad\mu((-\infty,x-\varepsilon])-\varepsilon\,\leq\,\mu_n((-\infty,x])\,\leq\,\mu((-\infty,x+\varepsilon])+\varepsilon
\end{align*}
for all $x\in\mathbb{R}$.\vspace{1em}\\
We conclude
\begin{align*}
\forall{}x\in\overline{\mathbb{R}}: \quad &\limsup\limits_{n\rightarrow\infty}\mu_n((-\infty,x])\leq\mu((-\infty,x]) \\[5pt] &\liminf\limits_{n\rightarrow\infty}\mu_n((-\infty,x])\geq\mu((-\infty,x]),
\end{align*}
and therefore 
\begin{align*}
\lim\limits_{n\rightarrow\infty}\mu_n((-\infty,x]) = \mu((-\infty,x]).
\end{align*}
Taking the complement of $(-\infty,x]$ then obviously yields
\begin{align*}
\forall{}x\in\overline{\mathbb{R}}:\quad\lim\limits_{n\rightarrow\infty}\mu_n((x,\infty)) = \mu((x,\infty)),
\end{align*}
and we obtain for arbitrary $a,b\in\mathbb{R}, a\leq{}b$
\begin{align*}
\lim\limits_{n\rightarrow\infty}\mu_n((a,b]) &= \lim\limits_{n\rightarrow\infty}\mu_n((-\infty,b]\cap{}(-\infty,a]^c) \\&= \lim\limits_{n\rightarrow\infty}\mu_n\left(\left((b,\infty)\dot{\cup}(-\infty,a]\right)^c\right) \\&= 1 - \lim\limits_{n\rightarrow\infty}\mu_n\left((b,\infty)\dot{\cup}(-\infty,a]\right) \\&= 1 - \lim\limits_{n\rightarrow\infty} \mu_n\left((b,\infty)\right) - \mu_n((-\infty,a]) \\&= 1 - \mu((b,\infty)) - \mu((-\infty,a]) \\&= \mu((a,b]).
\end{align*}
This yields weak convergence of the sequence $(\mu_n)_{n\in\mathbb{N}}$.
\end{proof}
\noindent{}Therefore $d_L$ induces the topology of weak convergence on $\mathcal{P}(\mathbb{R})$. Recall that given a Polish space $(\mathcal{X},d)$, the space $(\mathcal{P(X)},d_P)$ was also Polish. Now note that $d_P$ and $d_L$ induce the same topology on $\mathcal{P}(\mathbb{R})$, namely the topology of weak convergence. Since $\mathbb{R}$ endowed with the regular notion of distance, is obviously a Polish space then $(\mathcal{P}(\mathbb{R}),d_P) = (\mathcal{P}(\mathbb{R}),d_L)$ is too.\vspace{2em}\\
\section{Conclusion}
Let's recall the problem we established in the introduction. We asked ourselves, what the distance between Europe and Asia might be. This concept of distance was vague and unclear. The first probability metric, we introduced was the Wasserstein distance. The Wasserstein distance was derived from Monge's optimal transport problem and quantified the price of transportation from production units in one place to consumption units in another. Of course, this was only a particular interpretation of this problem. Since the Wasserstein distance was defined using the metric as a cost function, it gives us an idea of the distance between two, by probability measures described spaces. The idea behind it was to lend weight to different places within the given spaces and to then take the distance between the two. In the upper example, this could for instance be done, by taking the distance between every square metre in Asia and in Europe and to then average out these distances. The Wasserstein distance takes the infimum over all ways to weigh these individual points. The whole point is, that this is done by couplings, which keep the spatial distributions of Europe and Asia intact. The minimising coupling of the two continents, would then be given by a probability measure that lends more weight to eastern parts of Europe and western parts of Asia. Therefore the Wasserstein metric gives us a very useful approach to distance between spatial distributions. The Wasserstein distance metrizes weak convergence. In the example above this could be interpreted  as follows. If the spatial distributions converge, so does the distance between the objects that are characterised by them. The only downside to this is that one has to reduce the space of probability metrics, to a subspace. Nevertheless the Wasserstein distance has additional nice properties, such as the transference of properties of the underlying metric space onto the Wasserstein space.\vspace{1em}\\
The next concept of distance was given by the Total Variation distance. Total Variation is considered the least useful of the set of metrics we introduced here. Under the condition of boundedness it provides an upper bound to the Wasserstein distance and is therefore stronger than Wasserstein. For a finite space it is also a lower bound to $W_p$. Under these restrictive conditions it metrizes weak convergence. Another aspect of Total Variation is that it can also be interpreted as a particular case of the Wasserstein distance, using the discrete metric. An interesting conclusion to come to is that given a separable space, Total Variation metrizes weak convergence on the space of all probability measures that are defined on the power set. This conclusion isn't really that useful, since this would also follow directly from the Wasserstein distance. Therefore Total Variation is probably the least common probability distance.\vspace{1em}\\
The next probability metric given was the Prokhorov distance. This is arguably the most useful probability metric. In terms of the example problem, given in the introduction it can be interpreted as the smallest value, by which we can increase the radius of any place in Asia such that the weight given to said space differs from the weight of any place in Europe by less than said value. Prokhorov metrized weak convergence on the space of all probability measures on the $\sigma$-algebra of a Polish metric space. Additionally we proved that this space, endowed with the Prokhorov metric is also Polish. The useful aspect of this metric, was that it had all of the properties that the Wasserstein metric had on the Wasserstein space, without having to restrict it to a subspace. The downside to this metric is the fact, that it is not easy to compute. Hence it is theroetically important, but in practice the Wasserstein metric proves itself more useful. \vspace{1em}\\
One other metric we introduced was the Lévy metric, which was an equivalent version of the Prokhorov metric on $\mathbb{R}$. This metric had the seame properties as the Prokhorov metric did on $\mathbb{R}$. Hence the space of probability measures on the euclidean line is a Polish space.     
\section{Outlook on miscellaneous probability distances}
We have synopsised some of the most important probability distances. As mentioned above, there is a large variety of others to be discovered. Before we finish, we give the reader a brief overview of some other probability metrics and their applications. 
\subsection*{Kolmogorov metric}
Given two probability measures $\mu,\nu\in\mathcal{P}(\mathbb{R})$ with respective distribution functions $F$ and $G$, then the Kolmogorov metric is given by
\begin{align*}
d_K(\mu,\nu):=\sup\limits_{x\in\mathbb{R}}|F(x)-G(x)|.
\end{align*}
Obviously it is $d_K\leq{}1$ and the Kolmogorov metric bounds the Lévy metric from above, i.e
\begin{align*}
d_L(\mu,\nu) \leq d_K(\mu,\nu).
\end{align*}
This means that convergence in the topology induced by $d_K$ implies weak convergence and that the topology given by $d_K$ is greater than the topology of weak convergence on $\mathcal{P}(\mathbb{R})$.\vspace{1em}\\
An inversed inequality exists as well, given by
\begin{align*}
d_K(\mu,\nu) \leq \left(1+\sup\limits_{x\in\mathbb{R}}|G'(x)|\right)d_L(\mu,\nu), 
\end{align*}
whereby absolute continuity of $G$ with respect to the lebesgue measure is required. This would mean, restriciting the space of probability measures to the set of all probability measures with absolutely continuous distribution functions, endowed with the topology of weak convergence can be metrized by the Kolmogorov metric.
\subsection*{Discrepancy metric}
Given a metric space $(\mathcal{X},d)$, the discrepancy metric between two probability measures $\mu,\nu\in\mathcal{P(X)}$ is given by 
\begin{align*}
d_D(\mu,\nu):=\sup\bigg\lbrace{}\mu(B)-\nu(B)\,;\,B\subset\mathcal{X}\text{ closed}\bigg\rbrace.
\end{align*}
$d_D$ assumes values in $[0,1]$. One can see that the discrepancy metric depends on the metric $d$ of the underlying space $\mathcal{X}$, though it is scale-invariant, since multiplying a metric by a factor yields the same topology, containing the same closed balls over which the supremum is taken.
The discrepancy metric satisfies a variety of inequalities, including being bounded from above by the Wasserstein metric.\vspace{2em}\\
If $\mathcal{X}$ is finite, then
\begin{align*}
d_{\text{min}}d_D \leq d_W,
\end{align*}  
where, as before $d_{\text{min}}:=\min\limits_{x\neq{}y}d(x,y)$ denotes the smallest distance between two distinct points in $\mathcal{X}$\\
Note that the finiteness was needed in order for $d_{\text{min}}\geq{}0$ to be strictly greater than zero. \vspace{2em}\\
For $\mathcal{X}=\mathbb{R}$ it holds 
\begin{align*}
d_K(\mu,\nu)\leq{}d_D(\mu,\nu)\leq{}2d_K(\mu,\nu).
\vspace{2em}\\
\end{align*}
The discrepancy metric is also directly bounded by Total Variation, $d_D\leq\|\cdot\|_{TV}$.\vspace{1em}
Another interesting and useful metric is given by the Hellinger distance.
\subsection*{The Hellinger distance}
Let $(\mathcal{X,F})$ be a measurable space. Given two probability measures $\mu,\nu\in\mathcal{P(X)}$ with distribution functions $f$ and $g$ with respect to a dominating measure $\lambda$, i.e 
\begin{align*}
&\quad\mu(A) = \int\limits_{A}f(x)d\lambda(x) \\\text{and}&\\&\quad\nu(B) = \int\limits_{B}g(x)d\lambda(x),
\end{align*}
then the Hellinger distance is given by 
\begin{align*}
\left(\int\limits_{\mathcal{X}}\left(\sqrt{f}-\sqrt{g}\right)^2d\lambda\right)^{\frac{1}{2}}.
\end{align*}
An important aspect of $d_H$ is that Total Variation induces the same topology on $\mathcal{P(X)}$ as the Hellinger distance, which means that for a finite Polish space $(\mathcal{X},d)$ , the Hellinger distance $d_H$ induces the topology of weak convergence on $(\mathcal{P(X)},d_H).$\vspace{2em}\\
\newpage
\begin{thebibliography}{9}
\bibitem{Alsion}
Alsion, L., Gibbs, and Edward, F. SU. (2002)
\emph{On choosing and bounding probability metrics,} pp.419-435, in
\emph{International Statistical Review,} vol. 70, no. 3. 
\bibitem{Barrio}
Barrio, E.D., Cuesta-Albertos, J.A., and Matran, C. (1999)
\emph{Tests of goodness of fit based on the $L_2$-Wasserstein distance,} pp.1230-1239, in \emph{The Annals of Statistics,} vol. 27, no. 4. 
\bibitem{Basso}
Basso, G. (2015)
\emph{A Hitchhiker's guide to Wasserstein distances.}
\bibitem{Billingsley}
Billingsley, P. (2013)
\emph{Convergence of probability measures,} John Wiley \& Sons, Inc.
\bibitem{Bobkov}
Bobkov, S., and Ledoux, M. (2014)
\emph{One-dimensional empirical measures, order statistics and Kantorovich transport distances, preprint.}
\bibitem{Bolley}
Bolley, F. (2008)
\emph{Separability and completeness for the Wasserstein distance,} pp.371-377, in
\emph{Séminaire de probabilités XLI,} Springer Berlin Heidelberg.
\bibitem{Cabrelli}
Cabrelli, C. A., and Molter, U. M. (1995)
\emph{The Kantorovich metric for probability measures on the circle,} pp.345-361, in
\emph{Journal of Computational and Applied mathematics,} 57.3.
\bibitem{den Hollander}
den Hollander, F. (2012)
\emph{Probability Theory: The coupling method,} in
\emph{Lecture notes.}
\bibitem{Dudley}
Dudley, R.M. (2010)
\emph{Distances of probability measures and random variables,} pp. 28-37, in
\emph{Selected Works of RM Dudley.} Springer New York.
\bibitem{Givens}
Givens, C. R., and Shortt, R. M. (1984)
\emph{A class of Wasserstein metrics for probability distributions,} pp.231-240, in
\emph{The Michigan Mathematical Journal,} 31.2.
\bibitem{Kloeckner}
Kloeckner, B. (2008)
\emph{A geometric study of Wasserstein spaces: Euclidean spaces,} in  \emph{arXiv preprint arXiv: 0804.3505.}
\bibitem{Kupper}
Kupper, M. (2015)
\emph{Stochastics II,} in
\emph{Lecture notes.}
\bibitem{Monge}
Monge, G. (1781)
\emph{Mémoire sur la théorie des déblais et des remblais,} in \emph{Historie de l'Académie Royale des Sciences de Paris.}
\bibitem{Rachev}
Rachev, S.T., Klebanov, L., Stoyanov, S.V., and Fabozzi, F. (2013)
\emph{The methods of distances in the theory of probability and statistics,} pp.11-31, Springer Science \& Business Media.
\bibitem{Villani}
Villani,C. (2008)
\emph{Optimal transport, old and new,} pp.41-123, Springer Verlag.
\vspace{1em}\\
\end{thebibliography}

\section*{Acknowledgements}
This manuscript contains a Bachelor thesis, written at the University of Konstanz during the winter semester 2016/17, supervised by Michael Kupper. I would like to thank Michael Kupper and Daniel Bartl for their support and supervision in writing this manuscript.\\
{\tt alexander.stannat@gmail.com}
\newpage
\section*{Erklärung der Selbstständigkeit}
Ich versichere hiermit, dass ich die vorliegende Arbeit zum Thema 
\begin{center}
{\sl Distances and metrics on probability measures }
\end{center}
selbstständig verfasst und keine anderen Hilfsmittel als die angegebenen benutzt habe. \\Die Stellen, die anderen Werken dem Wortlaut oder dem Sinne nach entnommen sind, habe ich in jedem einzelnen Fall durch Angaben von Quelle, auch der benutzten Sekundärliteratur,
als Entlehnung kenntlich gemacht.\\Die Arbeit wurde bisher keiner anderen Prüfungsbehörde vorgelegt und auch noch nicht\\
veröffentlicht.\vspace{5em}\\
Konstanz, 16.03.2017\hspace{10em}Alexander Stannat





\end{document}